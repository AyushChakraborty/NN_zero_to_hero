{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making a trigram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn',\n",
       " 'abigail',\n",
       " 'emily',\n",
       " 'elizabeth',\n",
       " 'mila',\n",
       " 'ella',\n",
       " 'avery',\n",
       " 'sofia',\n",
       " 'camila',\n",
       " 'aria',\n",
       " 'scarlett',\n",
       " 'victoria',\n",
       " 'madison',\n",
       " 'luna',\n",
       " 'grace',\n",
       " 'chloe',\n",
       " 'penelope',\n",
       " 'layla',\n",
       " 'riley',\n",
       " 'zoey',\n",
       " 'nora',\n",
       " 'lily',\n",
       " 'eleanor',\n",
       " 'hannah',\n",
       " 'lillian',\n",
       " 'addison',\n",
       " 'aubrey',\n",
       " 'ellie',\n",
       " 'stella',\n",
       " 'natalie',\n",
       " 'zoe',\n",
       " 'leah',\n",
       " 'hazel',\n",
       " 'violet',\n",
       " 'aurora',\n",
       " 'savannah',\n",
       " 'audrey',\n",
       " 'brooklyn',\n",
       " 'bella',\n",
       " 'claire',\n",
       " 'skylar',\n",
       " 'lucy',\n",
       " 'paisley',\n",
       " 'everly',\n",
       " 'anna',\n",
       " 'caroline',\n",
       " 'nova',\n",
       " 'genesis',\n",
       " 'emilia',\n",
       " 'kennedy',\n",
       " 'samantha',\n",
       " 'maya',\n",
       " 'willow',\n",
       " 'kinsley',\n",
       " 'naomi',\n",
       " 'aaliyah',\n",
       " 'elena',\n",
       " 'sarah',\n",
       " 'ariana',\n",
       " 'allison',\n",
       " 'gabriella',\n",
       " 'alice',\n",
       " 'madelyn',\n",
       " 'cora',\n",
       " 'ruby',\n",
       " 'eva',\n",
       " 'serenity',\n",
       " 'autumn',\n",
       " 'adeline',\n",
       " 'hailey',\n",
       " 'gianna',\n",
       " 'valentina',\n",
       " 'isla',\n",
       " 'eliana',\n",
       " 'quinn',\n",
       " 'nevaeh',\n",
       " 'ivy',\n",
       " 'sadie',\n",
       " 'piper',\n",
       " 'lydia',\n",
       " 'alexa',\n",
       " 'josephine',\n",
       " 'emery',\n",
       " 'julia',\n",
       " 'delilah',\n",
       " 'arianna',\n",
       " 'vivian',\n",
       " 'kaylee',\n",
       " 'sophie',\n",
       " 'brielle',\n",
       " 'madeline',\n",
       " 'peyton',\n",
       " 'rylee',\n",
       " 'clara',\n",
       " 'hadley',\n",
       " 'melanie',\n",
       " 'mackenzie',\n",
       " 'reagan',\n",
       " 'adalynn',\n",
       " 'liliana',\n",
       " 'aubree',\n",
       " 'jade',\n",
       " 'katherine',\n",
       " 'isabelle',\n",
       " 'natalia',\n",
       " 'raelynn',\n",
       " 'maria',\n",
       " 'athena',\n",
       " 'ximena',\n",
       " 'arya',\n",
       " 'leilani',\n",
       " 'taylor',\n",
       " 'faith',\n",
       " 'rose',\n",
       " 'kylie',\n",
       " 'alexandra',\n",
       " 'mary',\n",
       " 'margaret',\n",
       " 'lyla',\n",
       " 'ashley',\n",
       " 'amaya',\n",
       " 'eliza',\n",
       " 'brianna',\n",
       " 'bailey',\n",
       " 'andrea',\n",
       " 'khloe',\n",
       " 'jasmine',\n",
       " 'melody',\n",
       " 'iris',\n",
       " 'isabel',\n",
       " 'norah',\n",
       " 'annabelle',\n",
       " 'valeria',\n",
       " 'emerson',\n",
       " 'adalyn',\n",
       " 'ryleigh',\n",
       " 'eden',\n",
       " 'emersyn',\n",
       " 'anastasia',\n",
       " 'kayla',\n",
       " 'alyssa',\n",
       " 'juliana',\n",
       " 'charlie',\n",
       " 'esther',\n",
       " 'ariel',\n",
       " 'cecilia',\n",
       " 'valerie',\n",
       " 'alina',\n",
       " 'molly',\n",
       " 'reese',\n",
       " 'aliyah',\n",
       " 'lilly',\n",
       " 'parker',\n",
       " 'finley',\n",
       " 'morgan',\n",
       " 'sydney',\n",
       " 'jordyn',\n",
       " 'eloise',\n",
       " 'trinity',\n",
       " 'daisy',\n",
       " 'kimberly',\n",
       " 'lauren',\n",
       " 'genevieve',\n",
       " 'sara',\n",
       " 'arabella',\n",
       " 'harmony',\n",
       " 'elise',\n",
       " 'remi',\n",
       " 'teagan',\n",
       " 'alexis',\n",
       " 'london',\n",
       " 'sloane',\n",
       " 'laila',\n",
       " 'lucia',\n",
       " 'diana',\n",
       " 'juliette',\n",
       " 'sienna',\n",
       " 'elliana',\n",
       " 'londyn',\n",
       " 'ayla',\n",
       " 'callie',\n",
       " 'gracie',\n",
       " 'josie',\n",
       " 'amara',\n",
       " 'jocelyn',\n",
       " 'daniela',\n",
       " 'everleigh',\n",
       " 'mya',\n",
       " 'rachel',\n",
       " 'summer',\n",
       " 'alana',\n",
       " 'brooke',\n",
       " 'alaina',\n",
       " 'mckenzie',\n",
       " 'catherine',\n",
       " 'amy',\n",
       " 'presley',\n",
       " 'journee',\n",
       " 'rosalie',\n",
       " 'ember',\n",
       " 'brynlee',\n",
       " 'rowan',\n",
       " 'joanna',\n",
       " 'paige',\n",
       " 'rebecca',\n",
       " 'ana',\n",
       " 'sawyer',\n",
       " 'mariah',\n",
       " 'nicole',\n",
       " 'brooklynn',\n",
       " 'payton',\n",
       " 'marley',\n",
       " 'fiona',\n",
       " 'georgia',\n",
       " 'lila',\n",
       " 'harley',\n",
       " 'adelyn',\n",
       " 'alivia',\n",
       " 'noelle',\n",
       " 'gemma',\n",
       " 'vanessa',\n",
       " 'journey',\n",
       " 'makayla',\n",
       " 'angelina',\n",
       " 'adaline',\n",
       " 'catalina',\n",
       " 'alayna',\n",
       " 'julianna',\n",
       " 'leila',\n",
       " 'lola',\n",
       " 'adriana',\n",
       " 'june',\n",
       " 'juliet',\n",
       " 'jayla',\n",
       " 'river',\n",
       " 'tessa',\n",
       " 'lia',\n",
       " 'dakota',\n",
       " 'delaney',\n",
       " 'selena',\n",
       " 'blakely',\n",
       " 'ada',\n",
       " 'camille',\n",
       " 'zara',\n",
       " 'malia',\n",
       " 'hope',\n",
       " 'samara',\n",
       " 'vera',\n",
       " 'mckenna',\n",
       " 'briella',\n",
       " 'izabella',\n",
       " 'hayden',\n",
       " 'raegan',\n",
       " 'michelle',\n",
       " 'angela',\n",
       " 'ruth',\n",
       " 'freya',\n",
       " 'kamila',\n",
       " 'vivienne',\n",
       " 'aspen',\n",
       " 'olive',\n",
       " 'kendall',\n",
       " 'elaina',\n",
       " 'thea',\n",
       " 'kali',\n",
       " 'destiny',\n",
       " 'amiyah',\n",
       " 'evangeline',\n",
       " 'cali',\n",
       " 'blake',\n",
       " 'elsie',\n",
       " 'juniper',\n",
       " 'alexandria',\n",
       " 'myla',\n",
       " 'ariella',\n",
       " 'kate',\n",
       " 'mariana',\n",
       " 'lilah',\n",
       " 'charlee',\n",
       " 'daleyza',\n",
       " 'nyla',\n",
       " 'jane',\n",
       " 'maggie',\n",
       " 'zuri',\n",
       " 'aniyah',\n",
       " 'lucille',\n",
       " 'leia',\n",
       " 'melissa',\n",
       " 'adelaide',\n",
       " 'amina',\n",
       " 'giselle',\n",
       " 'lena',\n",
       " 'camilla',\n",
       " 'miriam',\n",
       " 'millie',\n",
       " 'brynn',\n",
       " 'gabrielle',\n",
       " 'sage',\n",
       " 'annie',\n",
       " 'logan',\n",
       " 'lilliana',\n",
       " 'haven',\n",
       " 'jessica',\n",
       " 'kaia',\n",
       " 'magnolia',\n",
       " 'amira',\n",
       " 'adelynn',\n",
       " 'makenzie',\n",
       " 'stephanie',\n",
       " 'nina',\n",
       " 'phoebe',\n",
       " 'arielle',\n",
       " 'evie',\n",
       " 'lyric',\n",
       " 'alessandra',\n",
       " 'gabriela',\n",
       " 'paislee',\n",
       " 'raelyn',\n",
       " 'madilyn',\n",
       " 'paris',\n",
       " 'makenna',\n",
       " 'kinley',\n",
       " 'gracelyn',\n",
       " 'talia',\n",
       " 'maeve',\n",
       " 'rylie',\n",
       " 'kiara',\n",
       " 'evelynn',\n",
       " 'brinley',\n",
       " 'jacqueline',\n",
       " 'laura',\n",
       " 'gracelynn',\n",
       " 'lexi',\n",
       " 'ariah',\n",
       " 'fatima',\n",
       " 'jennifer',\n",
       " 'kehlani',\n",
       " 'alani',\n",
       " 'ariyah',\n",
       " 'luciana',\n",
       " 'allie',\n",
       " 'heidi',\n",
       " 'maci',\n",
       " 'phoenix',\n",
       " 'felicity',\n",
       " 'joy',\n",
       " 'kenzie',\n",
       " 'veronica',\n",
       " 'margot',\n",
       " 'addilyn',\n",
       " 'lana',\n",
       " 'cassidy',\n",
       " 'remington',\n",
       " 'saylor',\n",
       " 'ryan',\n",
       " 'keira',\n",
       " 'harlow',\n",
       " 'miranda',\n",
       " 'angel',\n",
       " 'amanda',\n",
       " 'daniella',\n",
       " 'royalty',\n",
       " 'gwendolyn',\n",
       " 'ophelia',\n",
       " 'heaven',\n",
       " 'jordan',\n",
       " 'madeleine',\n",
       " 'esmeralda',\n",
       " 'kira',\n",
       " 'miracle',\n",
       " 'elle',\n",
       " 'amari',\n",
       " 'danielle',\n",
       " 'daphne',\n",
       " 'willa',\n",
       " 'haley',\n",
       " 'gia',\n",
       " 'kaitlyn',\n",
       " 'oakley',\n",
       " 'kailani',\n",
       " 'winter',\n",
       " 'alicia',\n",
       " 'serena',\n",
       " 'nadia',\n",
       " 'aviana',\n",
       " 'demi',\n",
       " 'jada',\n",
       " 'braelynn',\n",
       " 'dylan',\n",
       " 'ainsley',\n",
       " 'alison',\n",
       " 'camryn',\n",
       " 'avianna',\n",
       " 'bianca',\n",
       " 'skyler',\n",
       " 'scarlet',\n",
       " 'maddison',\n",
       " 'nylah',\n",
       " 'sarai',\n",
       " 'regina',\n",
       " 'dahlia',\n",
       " 'nayeli',\n",
       " 'raven',\n",
       " 'helen',\n",
       " 'adrianna',\n",
       " 'averie',\n",
       " 'skye',\n",
       " 'kelsey',\n",
       " 'tatum',\n",
       " 'kensley',\n",
       " 'maliyah',\n",
       " 'erin',\n",
       " 'viviana',\n",
       " 'jenna',\n",
       " 'anaya',\n",
       " 'carolina',\n",
       " 'shelby',\n",
       " 'sabrina',\n",
       " 'mikayla',\n",
       " 'annalise',\n",
       " 'octavia',\n",
       " 'lennon',\n",
       " 'blair',\n",
       " 'carmen',\n",
       " 'yaretzi',\n",
       " 'kennedi',\n",
       " 'mabel',\n",
       " 'zariah',\n",
       " 'kyla',\n",
       " 'christina',\n",
       " 'selah',\n",
       " 'celeste',\n",
       " 'eve',\n",
       " 'mckinley',\n",
       " 'milani',\n",
       " 'frances',\n",
       " 'jimena',\n",
       " 'kylee',\n",
       " 'leighton',\n",
       " 'katie',\n",
       " 'aitana',\n",
       " 'kayleigh',\n",
       " 'sierra',\n",
       " 'kathryn',\n",
       " 'rosemary',\n",
       " 'jolene',\n",
       " 'alondra',\n",
       " 'elisa',\n",
       " 'helena',\n",
       " 'charleigh',\n",
       " 'hallie',\n",
       " 'lainey',\n",
       " 'avah',\n",
       " 'jazlyn',\n",
       " 'kamryn',\n",
       " 'mira',\n",
       " 'cheyenne',\n",
       " 'francesca',\n",
       " 'antonella',\n",
       " 'wren',\n",
       " 'chelsea',\n",
       " 'amber',\n",
       " 'emory',\n",
       " 'lorelei',\n",
       " 'nia',\n",
       " 'abby',\n",
       " 'april',\n",
       " 'emelia',\n",
       " 'carter',\n",
       " 'aylin',\n",
       " 'cataleya',\n",
       " 'bethany',\n",
       " 'marlee',\n",
       " 'carly',\n",
       " 'kaylani',\n",
       " 'emely',\n",
       " 'liana',\n",
       " 'madelynn',\n",
       " 'cadence',\n",
       " 'matilda',\n",
       " 'sylvia',\n",
       " 'myra',\n",
       " 'fernanda',\n",
       " 'oaklyn',\n",
       " 'elianna',\n",
       " 'hattie',\n",
       " 'dayana',\n",
       " 'kendra',\n",
       " 'maisie',\n",
       " 'malaysia',\n",
       " 'kara',\n",
       " 'katelyn',\n",
       " 'maia',\n",
       " 'celine',\n",
       " 'cameron',\n",
       " 'renata',\n",
       " 'jayleen',\n",
       " 'charli',\n",
       " 'emmalyn',\n",
       " 'holly',\n",
       " 'azalea',\n",
       " 'leona',\n",
       " 'alejandra',\n",
       " 'bristol',\n",
       " 'collins',\n",
       " 'imani',\n",
       " 'meadow',\n",
       " 'alexia',\n",
       " 'edith',\n",
       " 'kaydence',\n",
       " 'leslie',\n",
       " 'lilith',\n",
       " 'kora',\n",
       " 'aisha',\n",
       " 'meredith',\n",
       " 'danna',\n",
       " 'wynter',\n",
       " 'emberly',\n",
       " 'julieta',\n",
       " 'michaela',\n",
       " 'alayah',\n",
       " 'jemma',\n",
       " 'reign',\n",
       " 'colette',\n",
       " 'kaliyah',\n",
       " 'elliott',\n",
       " 'johanna',\n",
       " 'remy',\n",
       " 'sutton',\n",
       " 'emmy',\n",
       " 'virginia',\n",
       " 'briana',\n",
       " 'oaklynn',\n",
       " 'adelina',\n",
       " 'everlee',\n",
       " 'megan',\n",
       " 'angelica',\n",
       " 'justice',\n",
       " 'mariam',\n",
       " 'khaleesi',\n",
       " 'macie',\n",
       " 'karsyn',\n",
       " 'alanna',\n",
       " 'aleah',\n",
       " 'mae',\n",
       " 'mallory',\n",
       " 'esme',\n",
       " 'skyla',\n",
       " 'madilynn',\n",
       " 'charley',\n",
       " 'allyson',\n",
       " 'hanna',\n",
       " 'shiloh',\n",
       " 'henley',\n",
       " 'macy',\n",
       " 'maryam',\n",
       " 'ivanna',\n",
       " 'ashlynn',\n",
       " 'lorelai',\n",
       " 'amora',\n",
       " 'ashlyn',\n",
       " 'sasha',\n",
       " 'baylee',\n",
       " 'beatrice',\n",
       " 'itzel',\n",
       " 'priscilla',\n",
       " 'marie',\n",
       " 'jayda',\n",
       " 'liberty',\n",
       " 'rory',\n",
       " 'alessia',\n",
       " 'alaia',\n",
       " 'janelle',\n",
       " 'kalani',\n",
       " 'gloria',\n",
       " 'sloan',\n",
       " 'dorothy',\n",
       " 'greta',\n",
       " 'julie',\n",
       " 'zahra',\n",
       " 'savanna',\n",
       " 'annabella',\n",
       " 'poppy',\n",
       " 'amalia',\n",
       " 'zaylee',\n",
       " 'cecelia',\n",
       " 'coraline',\n",
       " 'kimber',\n",
       " 'emmie',\n",
       " 'anne',\n",
       " 'karina',\n",
       " 'kassidy',\n",
       " 'kynlee',\n",
       " 'monroe',\n",
       " 'anahi',\n",
       " 'jaliyah',\n",
       " 'jazmin',\n",
       " 'maren',\n",
       " 'monica',\n",
       " 'siena',\n",
       " 'marilyn',\n",
       " 'reyna',\n",
       " 'kyra',\n",
       " 'lilian',\n",
       " 'jamie',\n",
       " 'melany',\n",
       " 'alaya',\n",
       " 'ariya',\n",
       " 'kelly',\n",
       " 'rosie',\n",
       " 'adley',\n",
       " 'dream',\n",
       " 'jaylah',\n",
       " 'laurel',\n",
       " 'jazmine',\n",
       " 'mina',\n",
       " 'karla',\n",
       " 'bailee',\n",
       " 'aubrie',\n",
       " 'katalina',\n",
       " 'melina',\n",
       " 'harlee',\n",
       " 'elliot',\n",
       " 'hayley',\n",
       " 'elaine',\n",
       " 'karen',\n",
       " 'dallas',\n",
       " 'irene',\n",
       " 'lylah',\n",
       " 'ivory',\n",
       " 'chaya',\n",
       " 'rosa',\n",
       " 'aleena',\n",
       " 'braelyn',\n",
       " 'nola',\n",
       " 'alma',\n",
       " 'leyla',\n",
       " 'pearl',\n",
       " 'addyson',\n",
       " 'roselyn',\n",
       " 'lacey',\n",
       " 'lennox',\n",
       " 'reina',\n",
       " 'aurelia',\n",
       " 'noa',\n",
       " 'janiyah',\n",
       " 'jessie',\n",
       " 'madisyn',\n",
       " 'saige',\n",
       " 'alia',\n",
       " 'tiana',\n",
       " 'astrid',\n",
       " 'cassandra',\n",
       " 'kyleigh',\n",
       " 'romina',\n",
       " 'stevie',\n",
       " 'haylee',\n",
       " 'zelda',\n",
       " 'lillie',\n",
       " 'aileen',\n",
       " 'brylee',\n",
       " 'eileen',\n",
       " 'yara',\n",
       " 'ensley',\n",
       " 'lauryn',\n",
       " 'giuliana',\n",
       " 'livia',\n",
       " 'anya',\n",
       " 'mikaela',\n",
       " 'palmer',\n",
       " 'lyra',\n",
       " 'mara',\n",
       " 'marina',\n",
       " 'kailey',\n",
       " 'liv',\n",
       " 'clementine',\n",
       " 'kenna',\n",
       " 'briar',\n",
       " 'emerie',\n",
       " 'galilea',\n",
       " 'tiffany',\n",
       " 'bonnie',\n",
       " 'elyse',\n",
       " 'cynthia',\n",
       " 'frida',\n",
       " 'kinslee',\n",
       " 'tatiana',\n",
       " 'joelle',\n",
       " 'armani',\n",
       " 'jolie',\n",
       " 'nalani',\n",
       " 'rayna',\n",
       " 'yareli',\n",
       " 'meghan',\n",
       " 'rebekah',\n",
       " 'addilynn',\n",
       " 'faye',\n",
       " 'zariyah',\n",
       " 'lea',\n",
       " 'aliza',\n",
       " 'julissa',\n",
       " 'lilyana',\n",
       " 'anika',\n",
       " 'kairi',\n",
       " 'aniya',\n",
       " 'noemi',\n",
       " 'angie',\n",
       " 'crystal',\n",
       " 'bridget',\n",
       " 'ari',\n",
       " 'davina',\n",
       " 'amelie',\n",
       " 'amirah',\n",
       " 'annika',\n",
       " 'elora',\n",
       " 'xiomara',\n",
       " 'linda',\n",
       " 'hana',\n",
       " 'laney',\n",
       " 'mercy',\n",
       " 'hadassah',\n",
       " 'madalyn',\n",
       " 'louisa',\n",
       " 'simone',\n",
       " 'kori',\n",
       " 'jillian',\n",
       " 'alena',\n",
       " 'malaya',\n",
       " 'miley',\n",
       " 'milan',\n",
       " 'sariyah',\n",
       " 'malani',\n",
       " 'clarissa',\n",
       " 'nala',\n",
       " 'princess',\n",
       " 'amani',\n",
       " 'analia',\n",
       " 'estella',\n",
       " 'milana',\n",
       " 'aya',\n",
       " 'chana',\n",
       " 'jayde',\n",
       " 'tenley',\n",
       " 'zaria',\n",
       " 'itzayana',\n",
       " 'penny',\n",
       " 'ailani',\n",
       " 'lara',\n",
       " 'aubriella',\n",
       " 'clare',\n",
       " 'lina',\n",
       " 'rhea',\n",
       " 'bria',\n",
       " 'thalia',\n",
       " 'keyla',\n",
       " 'haisley',\n",
       " 'ryann',\n",
       " 'addisyn',\n",
       " 'amaia',\n",
       " 'chanel',\n",
       " 'ellen',\n",
       " 'harmoni',\n",
       " 'aliana',\n",
       " 'tinsley',\n",
       " 'landry',\n",
       " 'paisleigh',\n",
       " 'lexie',\n",
       " 'myah',\n",
       " 'rylan',\n",
       " 'deborah',\n",
       " 'emilee',\n",
       " 'laylah',\n",
       " 'novalee',\n",
       " 'ellis',\n",
       " 'emmeline',\n",
       " 'avalynn',\n",
       " 'hadlee',\n",
       " 'legacy',\n",
       " 'braylee',\n",
       " 'elisabeth',\n",
       " 'kaylie',\n",
       " 'ansley',\n",
       " 'dior',\n",
       " 'paula',\n",
       " 'belen',\n",
       " 'corinne',\n",
       " 'maleah',\n",
       " 'martha',\n",
       " 'teresa',\n",
       " 'salma',\n",
       " 'louise',\n",
       " 'averi',\n",
       " 'lilianna',\n",
       " 'amiya',\n",
       " 'milena',\n",
       " 'royal',\n",
       " 'aubrielle',\n",
       " 'calliope',\n",
       " 'frankie',\n",
       " 'natasha',\n",
       " 'kamilah',\n",
       " 'meilani',\n",
       " 'raina',\n",
       " 'amayah',\n",
       " 'lailah',\n",
       " 'rayne',\n",
       " 'zaniyah',\n",
       " 'isabela',\n",
       " 'nathalie',\n",
       " 'miah',\n",
       " 'opal',\n",
       " 'kenia',\n",
       " 'azariah',\n",
       " 'hunter',\n",
       " 'tori',\n",
       " 'andi',\n",
       " 'keily',\n",
       " 'leanna',\n",
       " 'scarlette',\n",
       " 'jaelyn',\n",
       " 'saoirse',\n",
       " 'selene',\n",
       " 'dalary',\n",
       " 'lindsey',\n",
       " 'marianna',\n",
       " 'ramona',\n",
       " 'estelle',\n",
       " 'giovanna',\n",
       " 'holland',\n",
       " 'nancy',\n",
       " 'emmalynn',\n",
       " 'mylah',\n",
       " 'rosalee',\n",
       " 'sariah',\n",
       " 'zoie',\n",
       " 'blaire',\n",
       " 'lyanna',\n",
       " 'maxine',\n",
       " 'anais',\n",
       " 'dana',\n",
       " 'judith',\n",
       " 'kiera',\n",
       " 'jaelynn',\n",
       " 'noor',\n",
       " 'kai',\n",
       " 'adalee',\n",
       " 'oaklee',\n",
       " 'amaris',\n",
       " 'jaycee',\n",
       " 'belle',\n",
       " 'carolyn',\n",
       " 'della',\n",
       " 'karter',\n",
       " 'sky',\n",
       " 'treasure',\n",
       " 'vienna',\n",
       " 'jewel',\n",
       " 'rivka',\n",
       " 'rosalyn',\n",
       " 'alannah',\n",
       " 'ellianna',\n",
       " 'sunny',\n",
       " 'claudia',\n",
       " 'cara',\n",
       " 'hailee',\n",
       " 'estrella',\n",
       " 'harleigh',\n",
       " 'zhavia',\n",
       " 'alianna',\n",
       " 'brittany',\n",
       " 'jaylene',\n",
       " 'journi',\n",
       " 'marissa',\n",
       " 'mavis',\n",
       " 'iliana',\n",
       " 'jurnee',\n",
       " 'aislinn',\n",
       " 'alyson',\n",
       " 'elsa',\n",
       " 'kamiyah',\n",
       " 'kiana',\n",
       " 'lisa',\n",
       " 'arlette',\n",
       " 'kadence',\n",
       " 'kathleen',\n",
       " 'halle',\n",
       " 'erika',\n",
       " 'sylvie',\n",
       " 'adele',\n",
       " 'erica',\n",
       " 'veda',\n",
       " 'whitney',\n",
       " 'bexley',\n",
       " 'emmaline',\n",
       " 'guadalupe',\n",
       " 'august',\n",
       " 'brynleigh',\n",
       " 'gwen',\n",
       " 'promise',\n",
       " 'alisson',\n",
       " 'india',\n",
       " 'madalynn',\n",
       " 'paloma',\n",
       " 'patricia',\n",
       " 'samira',\n",
       " 'aliya',\n",
       " 'casey',\n",
       " 'jazlynn',\n",
       " 'paulina',\n",
       " 'dulce',\n",
       " 'kallie',\n",
       " 'perla',\n",
       " 'adrienne',\n",
       " 'alora',\n",
       " 'nataly',\n",
       " 'ayleen',\n",
       " 'christine',\n",
       " 'kaiya',\n",
       " 'ariadne',\n",
       " 'karlee',\n",
       " 'barbara',\n",
       " 'lillianna',\n",
       " 'raquel',\n",
       " 'saniyah',\n",
       " 'yamileth',\n",
       " 'arely',\n",
       " 'celia',\n",
       " 'heavenly',\n",
       " 'kaylin',\n",
       " 'marisol',\n",
       " 'marleigh',\n",
       " 'avalyn',\n",
       " 'berkley',\n",
       " 'kataleya',\n",
       " 'zainab',\n",
       " 'dani',\n",
       " 'egypt',\n",
       " 'joyce',\n",
       " 'kenley',\n",
       " 'annabel',\n",
       " 'kaelyn',\n",
       " 'etta',\n",
       " 'hadleigh',\n",
       " 'joselyn',\n",
       " 'luella',\n",
       " 'jaylee',\n",
       " 'zola',\n",
       " 'alisha',\n",
       " 'ezra',\n",
       " 'queen',\n",
       " 'amia',\n",
       " 'annalee',\n",
       " 'bellamy',\n",
       " 'paola',\n",
       " 'tinley',\n",
       " 'violeta',\n",
       " 'jenesis',\n",
       " 'arden',\n",
       " 'giana',\n",
       " 'wendy',\n",
       " 'ellison',\n",
       " 'florence',\n",
       " 'margo',\n",
       " 'naya',\n",
       " 'robin',\n",
       " 'sandra',\n",
       " 'scout',\n",
       " 'waverly',\n",
       " 'janessa',\n",
       " 'jayden',\n",
       " 'micah',\n",
       " 'novah',\n",
       " 'zora',\n",
       " 'ann',\n",
       " 'jana',\n",
       " 'taliyah',\n",
       " 'vada',\n",
       " 'giavanna',\n",
       " 'ingrid',\n",
       " 'valery',\n",
       " 'azaria',\n",
       " 'emmarie',\n",
       " 'esperanza',\n",
       " 'kailyn',\n",
       " 'aiyana',\n",
       " 'keilani',\n",
       " 'austyn',\n",
       " 'whitley',\n",
       " 'elina',\n",
       " 'kimora',\n",
       " 'maliah',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the count model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = []\n",
    "for word in words:\n",
    "    word = '.' + word + '.'\n",
    "    for ch1, ch2, ch3 in zip(word, word[1:], word[2:]):\n",
    "        trigrams.append(ch1 + ch2 + ch3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.em',\n",
       " 'emm',\n",
       " 'mma',\n",
       " 'ma.',\n",
       " '.ol',\n",
       " 'oli',\n",
       " 'liv',\n",
       " 'ivi',\n",
       " 'via',\n",
       " 'ia.',\n",
       " '.av',\n",
       " 'ava',\n",
       " 'va.',\n",
       " '.is',\n",
       " 'isa',\n",
       " 'sab',\n",
       " 'abe',\n",
       " 'bel',\n",
       " 'ell',\n",
       " 'lla',\n",
       " 'la.',\n",
       " '.so',\n",
       " 'sop',\n",
       " 'oph',\n",
       " 'phi',\n",
       " 'hia',\n",
       " 'ia.',\n",
       " '.ch',\n",
       " 'cha',\n",
       " 'har',\n",
       " 'arl',\n",
       " 'rlo',\n",
       " 'lot',\n",
       " 'ott',\n",
       " 'tte',\n",
       " 'te.',\n",
       " '.mi',\n",
       " 'mia',\n",
       " 'ia.',\n",
       " '.am',\n",
       " 'ame',\n",
       " 'mel',\n",
       " 'eli',\n",
       " 'lia',\n",
       " 'ia.',\n",
       " '.ha',\n",
       " 'har',\n",
       " 'arp',\n",
       " 'rpe',\n",
       " 'per',\n",
       " 'er.',\n",
       " '.ev',\n",
       " 'eve',\n",
       " 'vel',\n",
       " 'ely',\n",
       " 'lyn',\n",
       " 'yn.',\n",
       " '.ab',\n",
       " 'abi',\n",
       " 'big',\n",
       " 'iga',\n",
       " 'gai',\n",
       " 'ail',\n",
       " 'il.',\n",
       " '.em',\n",
       " 'emi',\n",
       " 'mil',\n",
       " 'ily',\n",
       " 'ly.',\n",
       " '.el',\n",
       " 'eli',\n",
       " 'liz',\n",
       " 'iza',\n",
       " 'zab',\n",
       " 'abe',\n",
       " 'bet',\n",
       " 'eth',\n",
       " 'th.',\n",
       " '.mi',\n",
       " 'mil',\n",
       " 'ila',\n",
       " 'la.',\n",
       " '.el',\n",
       " 'ell',\n",
       " 'lla',\n",
       " 'la.',\n",
       " '.av',\n",
       " 'ave',\n",
       " 'ver',\n",
       " 'ery',\n",
       " 'ry.',\n",
       " '.so',\n",
       " 'sof',\n",
       " 'ofi',\n",
       " 'fia',\n",
       " 'ia.',\n",
       " '.ca',\n",
       " 'cam',\n",
       " 'ami',\n",
       " 'mil',\n",
       " 'ila',\n",
       " 'la.',\n",
       " '.ar',\n",
       " 'ari',\n",
       " 'ria',\n",
       " 'ia.',\n",
       " '.sc',\n",
       " 'sca',\n",
       " 'car',\n",
       " 'arl',\n",
       " 'rle',\n",
       " 'let',\n",
       " 'ett',\n",
       " 'tt.',\n",
       " '.vi',\n",
       " 'vic',\n",
       " 'ict',\n",
       " 'cto',\n",
       " 'tor',\n",
       " 'ori',\n",
       " 'ria',\n",
       " 'ia.',\n",
       " '.ma',\n",
       " 'mad',\n",
       " 'adi',\n",
       " 'dis',\n",
       " 'iso',\n",
       " 'son',\n",
       " 'on.',\n",
       " '.lu',\n",
       " 'lun',\n",
       " 'una',\n",
       " 'na.',\n",
       " '.gr',\n",
       " 'gra',\n",
       " 'rac',\n",
       " 'ace',\n",
       " 'ce.',\n",
       " '.ch',\n",
       " 'chl',\n",
       " 'hlo',\n",
       " 'loe',\n",
       " 'oe.',\n",
       " '.pe',\n",
       " 'pen',\n",
       " 'ene',\n",
       " 'nel',\n",
       " 'elo',\n",
       " 'lop',\n",
       " 'ope',\n",
       " 'pe.',\n",
       " '.la',\n",
       " 'lay',\n",
       " 'ayl',\n",
       " 'yla',\n",
       " 'la.',\n",
       " '.ri',\n",
       " 'ril',\n",
       " 'ile',\n",
       " 'ley',\n",
       " 'ey.',\n",
       " '.zo',\n",
       " 'zoe',\n",
       " 'oey',\n",
       " 'ey.',\n",
       " '.no',\n",
       " 'nor',\n",
       " 'ora',\n",
       " 'ra.',\n",
       " '.li',\n",
       " 'lil',\n",
       " 'ily',\n",
       " 'ly.',\n",
       " '.el',\n",
       " 'ele',\n",
       " 'lea',\n",
       " 'ean',\n",
       " 'ano',\n",
       " 'nor',\n",
       " 'or.',\n",
       " '.ha',\n",
       " 'han',\n",
       " 'ann',\n",
       " 'nna',\n",
       " 'nah',\n",
       " 'ah.',\n",
       " '.li',\n",
       " 'lil',\n",
       " 'ill',\n",
       " 'lli',\n",
       " 'lia',\n",
       " 'ian',\n",
       " 'an.',\n",
       " '.ad',\n",
       " 'add',\n",
       " 'ddi',\n",
       " 'dis',\n",
       " 'iso',\n",
       " 'son',\n",
       " 'on.',\n",
       " '.au',\n",
       " 'aub',\n",
       " 'ubr',\n",
       " 'bre',\n",
       " 'rey',\n",
       " 'ey.',\n",
       " '.el',\n",
       " 'ell',\n",
       " 'lli',\n",
       " 'lie',\n",
       " 'ie.',\n",
       " '.st',\n",
       " 'ste',\n",
       " 'tel',\n",
       " 'ell',\n",
       " 'lla',\n",
       " 'la.',\n",
       " '.na',\n",
       " 'nat',\n",
       " 'ata',\n",
       " 'tal',\n",
       " 'ali',\n",
       " 'lie',\n",
       " 'ie.',\n",
       " '.zo',\n",
       " 'zoe',\n",
       " 'oe.',\n",
       " '.le',\n",
       " 'lea',\n",
       " 'eah',\n",
       " 'ah.',\n",
       " '.ha',\n",
       " 'haz',\n",
       " 'aze',\n",
       " 'zel',\n",
       " 'el.',\n",
       " '.vi',\n",
       " 'vio',\n",
       " 'iol',\n",
       " 'ole',\n",
       " 'let',\n",
       " 'et.',\n",
       " '.au',\n",
       " 'aur',\n",
       " 'uro',\n",
       " 'ror',\n",
       " 'ora',\n",
       " 'ra.',\n",
       " '.sa',\n",
       " 'sav',\n",
       " 'ava',\n",
       " 'van',\n",
       " 'ann',\n",
       " 'nna',\n",
       " 'nah',\n",
       " 'ah.',\n",
       " '.au',\n",
       " 'aud',\n",
       " 'udr',\n",
       " 'dre',\n",
       " 'rey',\n",
       " 'ey.',\n",
       " '.br',\n",
       " 'bro',\n",
       " 'roo',\n",
       " 'ook',\n",
       " 'okl',\n",
       " 'kly',\n",
       " 'lyn',\n",
       " 'yn.',\n",
       " '.be',\n",
       " 'bel',\n",
       " 'ell',\n",
       " 'lla',\n",
       " 'la.',\n",
       " '.cl',\n",
       " 'cla',\n",
       " 'lai',\n",
       " 'air',\n",
       " 'ire',\n",
       " 're.',\n",
       " '.sk',\n",
       " 'sky',\n",
       " 'kyl',\n",
       " 'yla',\n",
       " 'lar',\n",
       " 'ar.',\n",
       " '.lu',\n",
       " 'luc',\n",
       " 'ucy',\n",
       " 'cy.',\n",
       " '.pa',\n",
       " 'pai',\n",
       " 'ais',\n",
       " 'isl',\n",
       " 'sle',\n",
       " 'ley',\n",
       " 'ey.',\n",
       " '.ev',\n",
       " 'eve',\n",
       " 'ver',\n",
       " 'erl',\n",
       " 'rly',\n",
       " 'ly.',\n",
       " '.an',\n",
       " 'ann',\n",
       " 'nna',\n",
       " 'na.',\n",
       " '.ca',\n",
       " 'car',\n",
       " 'aro',\n",
       " 'rol',\n",
       " 'oli',\n",
       " 'lin',\n",
       " 'ine',\n",
       " 'ne.',\n",
       " '.no',\n",
       " 'nov',\n",
       " 'ova',\n",
       " 'va.',\n",
       " '.ge',\n",
       " 'gen',\n",
       " 'ene',\n",
       " 'nes',\n",
       " 'esi',\n",
       " 'sis',\n",
       " 'is.',\n",
       " '.em',\n",
       " 'emi',\n",
       " 'mil',\n",
       " 'ili',\n",
       " 'lia',\n",
       " 'ia.',\n",
       " '.ke',\n",
       " 'ken',\n",
       " 'enn',\n",
       " 'nne',\n",
       " 'ned',\n",
       " 'edy',\n",
       " 'dy.',\n",
       " '.sa',\n",
       " 'sam',\n",
       " 'ama',\n",
       " 'man',\n",
       " 'ant',\n",
       " 'nth',\n",
       " 'tha',\n",
       " 'ha.',\n",
       " '.ma',\n",
       " 'may',\n",
       " 'aya',\n",
       " 'ya.',\n",
       " '.wi',\n",
       " 'wil',\n",
       " 'ill',\n",
       " 'llo',\n",
       " 'low',\n",
       " 'ow.',\n",
       " '.ki',\n",
       " 'kin',\n",
       " 'ins',\n",
       " 'nsl',\n",
       " 'sle',\n",
       " 'ley',\n",
       " 'ey.',\n",
       " '.na',\n",
       " 'nao',\n",
       " 'aom',\n",
       " 'omi',\n",
       " 'mi.',\n",
       " '.aa',\n",
       " 'aal',\n",
       " 'ali',\n",
       " 'liy',\n",
       " 'iya',\n",
       " 'yah',\n",
       " 'ah.',\n",
       " '.el',\n",
       " 'ele',\n",
       " 'len',\n",
       " 'ena',\n",
       " 'na.',\n",
       " '.sa',\n",
       " 'sar',\n",
       " 'ara',\n",
       " 'rah',\n",
       " 'ah.',\n",
       " '.ar',\n",
       " 'ari',\n",
       " 'ria',\n",
       " 'ian',\n",
       " 'ana',\n",
       " 'na.',\n",
       " '.al',\n",
       " 'all',\n",
       " 'lli',\n",
       " 'lis',\n",
       " 'iso',\n",
       " 'son',\n",
       " 'on.',\n",
       " '.ga',\n",
       " 'gab',\n",
       " 'abr',\n",
       " 'bri',\n",
       " 'rie',\n",
       " 'iel',\n",
       " 'ell',\n",
       " 'lla',\n",
       " 'la.',\n",
       " '.al',\n",
       " 'ali',\n",
       " 'lic',\n",
       " 'ice',\n",
       " 'ce.',\n",
       " '.ma',\n",
       " 'mad',\n",
       " 'ade',\n",
       " 'del',\n",
       " 'ely',\n",
       " 'lyn',\n",
       " 'yn.',\n",
       " '.co',\n",
       " 'cor',\n",
       " 'ora',\n",
       " 'ra.',\n",
       " '.ru',\n",
       " 'rub',\n",
       " 'uby',\n",
       " 'by.',\n",
       " '.ev',\n",
       " 'eva',\n",
       " 'va.',\n",
       " '.se',\n",
       " 'ser',\n",
       " 'ere',\n",
       " 'ren',\n",
       " 'eni',\n",
       " 'nit',\n",
       " 'ity',\n",
       " 'ty.',\n",
       " '.au',\n",
       " 'aut',\n",
       " 'utu',\n",
       " 'tum',\n",
       " 'umn',\n",
       " 'mn.',\n",
       " '.ad',\n",
       " 'ade',\n",
       " 'del',\n",
       " 'eli',\n",
       " 'lin',\n",
       " 'ine',\n",
       " 'ne.',\n",
       " '.ha',\n",
       " 'hai',\n",
       " 'ail',\n",
       " 'ile',\n",
       " 'ley',\n",
       " 'ey.',\n",
       " '.gi',\n",
       " 'gia',\n",
       " 'ian',\n",
       " 'ann',\n",
       " 'nna',\n",
       " 'na.',\n",
       " '.va',\n",
       " 'val',\n",
       " 'ale',\n",
       " 'len',\n",
       " 'ent',\n",
       " 'nti',\n",
       " 'tin',\n",
       " 'ina',\n",
       " 'na.',\n",
       " '.is',\n",
       " 'isl',\n",
       " 'sla',\n",
       " 'la.',\n",
       " '.el',\n",
       " 'eli',\n",
       " 'lia',\n",
       " 'ian',\n",
       " 'ana',\n",
       " 'na.',\n",
       " '.qu',\n",
       " 'qui',\n",
       " 'uin',\n",
       " 'inn',\n",
       " 'nn.',\n",
       " '.ne',\n",
       " 'nev',\n",
       " 'eva',\n",
       " 'vae',\n",
       " 'aeh',\n",
       " 'eh.',\n",
       " '.iv',\n",
       " 'ivy',\n",
       " 'vy.',\n",
       " '.sa',\n",
       " 'sad',\n",
       " 'adi',\n",
       " 'die',\n",
       " 'ie.',\n",
       " '.pi',\n",
       " 'pip',\n",
       " 'ipe',\n",
       " 'per',\n",
       " 'er.',\n",
       " '.ly',\n",
       " 'lyd',\n",
       " 'ydi',\n",
       " 'dia',\n",
       " 'ia.',\n",
       " '.al',\n",
       " 'ale',\n",
       " 'lex',\n",
       " 'exa',\n",
       " 'xa.',\n",
       " '.jo',\n",
       " 'jos',\n",
       " 'ose',\n",
       " 'sep',\n",
       " 'eph',\n",
       " 'phi',\n",
       " 'hin',\n",
       " 'ine',\n",
       " 'ne.',\n",
       " '.em',\n",
       " 'eme',\n",
       " 'mer',\n",
       " 'ery',\n",
       " 'ry.',\n",
       " '.ju',\n",
       " 'jul',\n",
       " 'uli',\n",
       " 'lia',\n",
       " 'ia.',\n",
       " '.de',\n",
       " 'del',\n",
       " 'eli',\n",
       " 'lil',\n",
       " 'ila',\n",
       " 'lah',\n",
       " 'ah.',\n",
       " '.ar',\n",
       " 'ari',\n",
       " 'ria',\n",
       " 'ian',\n",
       " 'ann',\n",
       " 'nna',\n",
       " 'na.',\n",
       " '.vi',\n",
       " 'viv',\n",
       " 'ivi',\n",
       " 'via',\n",
       " 'ian',\n",
       " 'an.',\n",
       " '.ka',\n",
       " 'kay',\n",
       " 'ayl',\n",
       " 'yle',\n",
       " 'lee',\n",
       " 'ee.',\n",
       " '.so',\n",
       " 'sop',\n",
       " 'oph',\n",
       " 'phi',\n",
       " 'hie',\n",
       " 'ie.',\n",
       " '.br',\n",
       " 'bri',\n",
       " 'rie',\n",
       " 'iel',\n",
       " 'ell',\n",
       " 'lle',\n",
       " 'le.',\n",
       " '.ma',\n",
       " 'mad',\n",
       " 'ade',\n",
       " 'del',\n",
       " 'eli',\n",
       " 'lin',\n",
       " 'ine',\n",
       " 'ne.',\n",
       " '.pe',\n",
       " 'pey',\n",
       " 'eyt',\n",
       " 'yto',\n",
       " 'ton',\n",
       " 'on.',\n",
       " '.ry',\n",
       " 'ryl',\n",
       " 'yle',\n",
       " 'lee',\n",
       " 'ee.',\n",
       " '.cl',\n",
       " 'cla',\n",
       " 'lar',\n",
       " 'ara',\n",
       " 'ra.',\n",
       " '.ha',\n",
       " 'had',\n",
       " 'adl',\n",
       " 'dle',\n",
       " 'ley',\n",
       " 'ey.',\n",
       " '.me',\n",
       " 'mel',\n",
       " 'ela',\n",
       " 'lan',\n",
       " 'ani',\n",
       " 'nie',\n",
       " 'ie.',\n",
       " '.ma',\n",
       " 'mac',\n",
       " 'ack',\n",
       " 'cke',\n",
       " 'ken',\n",
       " 'enz',\n",
       " 'nzi',\n",
       " 'zie',\n",
       " 'ie.',\n",
       " '.re',\n",
       " 'rea',\n",
       " 'eag',\n",
       " 'aga',\n",
       " 'gan',\n",
       " 'an.',\n",
       " '.ad',\n",
       " 'ada',\n",
       " 'dal',\n",
       " 'aly',\n",
       " 'lyn',\n",
       " 'ynn',\n",
       " 'nn.',\n",
       " '.li',\n",
       " 'lil',\n",
       " 'ili',\n",
       " 'lia',\n",
       " 'ian',\n",
       " 'ana',\n",
       " 'na.',\n",
       " '.au',\n",
       " 'aub',\n",
       " 'ubr',\n",
       " 'bre',\n",
       " 'ree',\n",
       " 'ee.',\n",
       " '.ja',\n",
       " 'jad',\n",
       " 'ade',\n",
       " 'de.',\n",
       " '.ka',\n",
       " 'kat',\n",
       " 'ath',\n",
       " 'the',\n",
       " 'her',\n",
       " 'eri',\n",
       " 'rin',\n",
       " 'ine',\n",
       " 'ne.',\n",
       " '.is',\n",
       " 'isa',\n",
       " 'sab',\n",
       " 'abe',\n",
       " 'bel',\n",
       " 'ell',\n",
       " 'lle',\n",
       " 'le.',\n",
       " '.na',\n",
       " 'nat',\n",
       " 'ata',\n",
       " 'tal',\n",
       " 'ali',\n",
       " 'lia',\n",
       " 'ia.',\n",
       " '.ra',\n",
       " 'rae',\n",
       " 'ael',\n",
       " 'ely',\n",
       " 'lyn',\n",
       " 'ynn',\n",
       " 'nn.',\n",
       " '.ma',\n",
       " 'mar',\n",
       " 'ari',\n",
       " 'ria',\n",
       " 'ia.',\n",
       " '.at',\n",
       " 'ath',\n",
       " 'the',\n",
       " 'hen',\n",
       " 'ena',\n",
       " 'na.',\n",
       " '.xi',\n",
       " 'xim',\n",
       " 'ime',\n",
       " 'men',\n",
       " 'ena',\n",
       " 'na.',\n",
       " '.ar',\n",
       " 'ary',\n",
       " 'rya',\n",
       " 'ya.',\n",
       " '.le',\n",
       " 'lei',\n",
       " 'eil',\n",
       " 'ila',\n",
       " 'lan',\n",
       " 'ani',\n",
       " 'ni.',\n",
       " '.ta',\n",
       " 'tay',\n",
       " 'ayl',\n",
       " 'ylo',\n",
       " 'lor',\n",
       " 'or.',\n",
       " '.fa',\n",
       " 'fai',\n",
       " 'ait',\n",
       " 'ith',\n",
       " 'th.',\n",
       " '.ro',\n",
       " 'ros',\n",
       " 'ose',\n",
       " 'se.',\n",
       " '.ky',\n",
       " 'kyl',\n",
       " 'yli',\n",
       " 'lie',\n",
       " 'ie.',\n",
       " '.al',\n",
       " 'ale',\n",
       " 'lex',\n",
       " 'exa',\n",
       " 'xan',\n",
       " 'and',\n",
       " 'ndr',\n",
       " 'dra',\n",
       " 'ra.',\n",
       " '.ma',\n",
       " 'mar',\n",
       " 'ary',\n",
       " 'ry.',\n",
       " '.ma',\n",
       " 'mar',\n",
       " 'arg',\n",
       " 'rga',\n",
       " 'gar',\n",
       " 'are',\n",
       " 'ret',\n",
       " 'et.',\n",
       " '.ly',\n",
       " 'lyl',\n",
       " 'yla',\n",
       " 'la.',\n",
       " '.as',\n",
       " 'ash',\n",
       " 'shl',\n",
       " 'hle',\n",
       " 'ley',\n",
       " 'ey.',\n",
       " '.am',\n",
       " 'ama',\n",
       " 'may',\n",
       " 'aya',\n",
       " 'ya.',\n",
       " '.el',\n",
       " 'eli',\n",
       " 'liz',\n",
       " 'iza',\n",
       " 'za.',\n",
       " '.br',\n",
       " 'bri',\n",
       " 'ria',\n",
       " 'ian',\n",
       " 'ann',\n",
       " 'nna',\n",
       " 'na.',\n",
       " '.ba',\n",
       " 'bai',\n",
       " 'ail',\n",
       " 'ile',\n",
       " 'ley',\n",
       " 'ey.',\n",
       " '.an',\n",
       " 'and',\n",
       " 'ndr',\n",
       " 'dre',\n",
       " 'rea',\n",
       " 'ea.',\n",
       " '.kh',\n",
       " 'khl',\n",
       " 'hlo',\n",
       " 'loe',\n",
       " 'oe.',\n",
       " '.ja',\n",
       " 'jas',\n",
       " 'asm',\n",
       " 'smi',\n",
       " 'min',\n",
       " 'ine',\n",
       " 'ne.',\n",
       " '.me',\n",
       " 'mel',\n",
       " 'elo',\n",
       " 'lod',\n",
       " 'ody',\n",
       " 'dy.',\n",
       " '.ir',\n",
       " 'iri',\n",
       " 'ris',\n",
       " 'is.',\n",
       " '.is',\n",
       " 'isa',\n",
       " 'sab',\n",
       " 'abe',\n",
       " 'bel',\n",
       " 'el.',\n",
       " '.no',\n",
       " 'nor',\n",
       " 'ora',\n",
       " 'rah',\n",
       " 'ah.',\n",
       " '.an',\n",
       " 'ann',\n",
       " 'nna',\n",
       " 'nab',\n",
       " 'abe',\n",
       " 'bel',\n",
       " 'ell',\n",
       " 'lle',\n",
       " 'le.',\n",
       " '.va',\n",
       " 'val',\n",
       " 'ale',\n",
       " 'ler',\n",
       " 'eri',\n",
       " 'ria',\n",
       " 'ia.',\n",
       " '.em',\n",
       " 'eme',\n",
       " 'mer',\n",
       " 'ers',\n",
       " 'rso',\n",
       " 'son',\n",
       " 'on.',\n",
       " '.ad',\n",
       " 'ada',\n",
       " 'dal',\n",
       " 'aly',\n",
       " 'lyn',\n",
       " 'yn.',\n",
       " '.ry',\n",
       " 'ryl',\n",
       " 'yle',\n",
       " 'lei',\n",
       " 'eig',\n",
       " 'igh',\n",
       " 'gh.',\n",
       " '.ed',\n",
       " 'ede',\n",
       " 'den',\n",
       " 'en.',\n",
       " '.em',\n",
       " 'eme',\n",
       " 'mer',\n",
       " 'ers',\n",
       " 'rsy',\n",
       " 'syn',\n",
       " 'yn.',\n",
       " '.an',\n",
       " 'ana',\n",
       " 'nas',\n",
       " 'ast',\n",
       " 'sta',\n",
       " 'tas',\n",
       " 'asi',\n",
       " 'sia',\n",
       " 'ia.',\n",
       " '.ka',\n",
       " 'kay',\n",
       " 'ayl',\n",
       " 'yla',\n",
       " 'la.',\n",
       " '.al',\n",
       " 'aly',\n",
       " 'lys',\n",
       " 'yss',\n",
       " 'ssa',\n",
       " 'sa.',\n",
       " '.ju',\n",
       " 'jul',\n",
       " 'uli',\n",
       " 'lia',\n",
       " 'ian',\n",
       " 'ana',\n",
       " 'na.',\n",
       " '.ch',\n",
       " 'cha',\n",
       " 'har',\n",
       " 'arl',\n",
       " 'rli',\n",
       " 'lie',\n",
       " 'ie.',\n",
       " '.es',\n",
       " 'est',\n",
       " 'sth',\n",
       " 'the',\n",
       " 'her',\n",
       " 'er.',\n",
       " '.ar',\n",
       " 'ari',\n",
       " 'rie',\n",
       " 'iel',\n",
       " 'el.',\n",
       " '.ce',\n",
       " 'cec',\n",
       " 'eci',\n",
       " 'cil',\n",
       " 'ili',\n",
       " 'lia',\n",
       " 'ia.',\n",
       " '.va',\n",
       " 'val',\n",
       " 'ale',\n",
       " 'ler',\n",
       " 'eri',\n",
       " 'rie',\n",
       " 'ie.',\n",
       " '.al',\n",
       " 'ali',\n",
       " 'lin',\n",
       " 'ina',\n",
       " 'na.',\n",
       " '.mo',\n",
       " 'mol',\n",
       " 'oll',\n",
       " 'lly',\n",
       " 'ly.',\n",
       " '.re',\n",
       " 'ree',\n",
       " 'ees',\n",
       " 'ese',\n",
       " 'se.',\n",
       " '.al',\n",
       " 'ali',\n",
       " 'liy',\n",
       " 'iya',\n",
       " 'yah',\n",
       " 'ah.',\n",
       " '.li',\n",
       " 'lil',\n",
       " 'ill',\n",
       " 'lly',\n",
       " 'ly.',\n",
       " '.pa',\n",
       " 'par',\n",
       " 'ark',\n",
       " 'rke',\n",
       " 'ker',\n",
       " 'er.',\n",
       " '.fi',\n",
       " 'fin',\n",
       " 'inl',\n",
       " 'nle',\n",
       " 'ley',\n",
       " 'ey.',\n",
       " '.mo',\n",
       " 'mor',\n",
       " 'org',\n",
       " 'rga',\n",
       " 'gan',\n",
       " 'an.',\n",
       " '.sy',\n",
       " 'syd',\n",
       " 'ydn',\n",
       " 'dne',\n",
       " 'ney',\n",
       " 'ey.',\n",
       " '.jo',\n",
       " 'jor',\n",
       " 'ord',\n",
       " 'rdy',\n",
       " 'dyn',\n",
       " 'yn.',\n",
       " '.el',\n",
       " 'elo',\n",
       " 'loi',\n",
       " 'ois',\n",
       " 'ise',\n",
       " 'se.',\n",
       " '.tr',\n",
       " 'tri',\n",
       " 'rin',\n",
       " 'ini',\n",
       " 'nit',\n",
       " 'ity',\n",
       " 'ty.',\n",
       " '.da',\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "atoi = {ch: i+1 for i, ch in enumerate(chars)}\n",
    "atoi['.'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '.': 0}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_array = torch.zeros((27, 27, 27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    for ch1, ch2, ch3 in zip(word, word[1:], word[2:]):\n",
    "        trigram_array[atoi[ch1], atoi[ch2], atoi[ch3]] += 1\n",
    "trigram_array += 1   #model smoothening by adding 1 to all counts, in case of model smoothening in count models\n",
    "#the term is known as laplace smoothening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  1,   1,   1,  ...,   1,   1,   1],\n",
       "         [  1,   1,   1,  ...,   1,   1,   1],\n",
       "         [  1,   1,   1,  ...,   1,   1,   1],\n",
       "         ...,\n",
       "         [  1,   1,   1,  ...,   1,   1,   1],\n",
       "         [  1,   1,   1,  ...,   1,   1,   1],\n",
       "         [  1,   1,   1,  ...,   1,   1,   1]],\n",
       "\n",
       "        [[  1,   1,   1,  ...,   1,   1,   1],\n",
       "         [  1,   1,   6,  ...,   1,  21,  12],\n",
       "         [  1,  29,  21,  ...,   1,  13,   1],\n",
       "         ...,\n",
       "         [  1,   6,   1,  ...,  18,   7,   4],\n",
       "         [  1, 390,  14,  ...,   1,  17,  41],\n",
       "         [  1, 124,   1,  ...,   1,  13,  23]],\n",
       "\n",
       "        [[  1,   1,   1,  ...,   1,   1,   1],\n",
       "         [  1,   6,   6,  ...,   5,  32,   5],\n",
       "         [  1,   9,   1,  ...,   1,  10,   1],\n",
       "         ...,\n",
       "         [  1,   1,   1,  ...,   1,   1,   1],\n",
       "         [  1,   5,   2,  ...,   1,   1,   1],\n",
       "         [  1,   1,   1,  ...,   1,   1,   1]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  1,   1,   1,  ...,   1,   1,   1],\n",
       "         [  1,   1,   3,  ...,   1,  11,   1],\n",
       "         [  1,   1,   1,  ...,   1,   2,   1],\n",
       "         ...,\n",
       "         [  1,   4,   1,  ...,   1,   2,   1],\n",
       "         [  1,   5,   1,  ...,   1,   1,   1],\n",
       "         [  1,  17,   1,  ...,   1,   1,   1]],\n",
       "\n",
       "        [[  1,   1,   1,  ...,   1,   1,   1],\n",
       "         [  1,  47,  11,  ...,   4,   7,  22],\n",
       "         [  1,   3,   1,  ...,   1,   1,   1],\n",
       "         ...,\n",
       "         [  1,   2,   1,  ...,   2,   1,   1],\n",
       "         [  1,  19,   1,  ...,   1,   1,   1],\n",
       "         [  1,  28,   1,  ...,   2,   1,   1]],\n",
       "\n",
       "        [[  1,   1,   1,  ...,   1,   1,   1],\n",
       "         [  1,  15,  41,  ...,   4,  98,   4],\n",
       "         [  1,   1,   1,  ...,   1,   1,   1],\n",
       "         ...,\n",
       "         [  1,   1,   1,  ...,   1,   1,   1],\n",
       "         [  1,  28,   1,  ...,   1,   1,   2],\n",
       "         [  1,  14,   1,  ...,   1,   8,   1]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(trigram_array[atoi['a'], atoi['b'], atoi['c']].item())  #an egs of count, in this case\n",
    "#egs of abc, and we see c follows ab once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since we have our count tensor, we can make predictions, here we use normal random sampling \n",
    "#based on prob of chars following each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the probability tensor \n",
    "P = trigram_array.float()/trigram_array.sum(2, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370],\n",
       "         [0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370],\n",
       "         [0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370],\n",
       "         ...,\n",
       "         [0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370],\n",
       "         [0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370],\n",
       "         [0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370]],\n",
       "\n",
       "        [[0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370],\n",
       "         [0.0018, 0.0018, 0.0110,  ..., 0.0018, 0.0387, 0.0221],\n",
       "         [0.0019, 0.0545, 0.0395,  ..., 0.0019, 0.0244, 0.0019],\n",
       "         ...,\n",
       "         [0.0051, 0.0303, 0.0051,  ..., 0.0909, 0.0354, 0.0202],\n",
       "         [0.0005, 0.2038, 0.0073,  ..., 0.0005, 0.0089, 0.0214],\n",
       "         [0.0024, 0.2925, 0.0024,  ..., 0.0024, 0.0307, 0.0542]],\n",
       "\n",
       "        [[0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370],\n",
       "         [0.0033, 0.0199, 0.0199,  ..., 0.0166, 0.1060, 0.0166],\n",
       "         [0.0156, 0.1406, 0.0156,  ..., 0.0156, 0.1562, 0.0156],\n",
       "         ...,\n",
       "         [0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370],\n",
       "         [0.0182, 0.0909, 0.0364,  ..., 0.0182, 0.0182, 0.0182],\n",
       "         [0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370],\n",
       "         [0.0083, 0.0083, 0.0250,  ..., 0.0083, 0.0917, 0.0083],\n",
       "         [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0714, 0.0357],\n",
       "         ...,\n",
       "         [0.0213, 0.0851, 0.0213,  ..., 0.0213, 0.0426, 0.0213],\n",
       "         [0.0192, 0.0962, 0.0192,  ..., 0.0192, 0.0192, 0.0192],\n",
       "         [0.0217, 0.3696, 0.0217,  ..., 0.0217, 0.0217, 0.0217]],\n",
       "\n",
       "        [[0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370],\n",
       "         [0.0007, 0.0323, 0.0076,  ..., 0.0028, 0.0048, 0.0151],\n",
       "         [0.0192, 0.0577, 0.0192,  ..., 0.0192, 0.0192, 0.0192],\n",
       "         ...,\n",
       "         [0.0312, 0.0625, 0.0312,  ..., 0.0625, 0.0312, 0.0312],\n",
       "         [0.0204, 0.3878, 0.0204,  ..., 0.0204, 0.0204, 0.0204],\n",
       "         [0.0097, 0.2718, 0.0097,  ..., 0.0194, 0.0097, 0.0097]],\n",
       "\n",
       "        [[0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370],\n",
       "         [0.0013, 0.0190, 0.0520,  ..., 0.0051, 0.1242, 0.0051],\n",
       "         [0.0323, 0.0323, 0.0323,  ..., 0.0323, 0.0323, 0.0323],\n",
       "         ...,\n",
       "         [0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370],\n",
       "         [0.0071, 0.2000, 0.0071,  ..., 0.0071, 0.0071, 0.0143],\n",
       "         [0.0147, 0.2059, 0.0147,  ..., 0.0147, 0.1176, 0.0147]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P  #find the prob of each char occuring after its prev two chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we see that a lot of the probs are uniform(after the smoothening), since we added 1 to all counts \n",
    "#since we use two chars and then look at the next one's prob, lots of pairs get \n",
    "#wasted since they are not in the dataset, as we have lots of \n",
    "#pairs possible but only some that are actually namelike, so \n",
    "#if we start from those pairs, we will get better results and by that\n",
    "#i mean shorter more namelike results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "itoa = {i: ch for ch, i in atoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itoa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_names = []\n",
    "for _ in range(5):\n",
    "    char = []\n",
    "    ix1 = 0\n",
    "    ix2 = atoi['e'] #generating only those words which start with a\n",
    "    char.append(itoa[ix2])\n",
    "    temp = 0\n",
    "    while True:\n",
    "        temp = ix2\n",
    "        ix2 = torch.multinomial((P[ix1, ix2]), num_samples=1, replacement=True, generator=g).item()\n",
    "        ix1 = temp\n",
    "        char.append(itoa[ix2])\n",
    "        if ix2==0:\n",
    "            break;\n",
    "    new_names.append(''.join(char))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ef.',\n",
       " 'evanisaleimbellinahmissephosperomilyzleyonelayelileyarmeraylimianasjpmccztkfcgc.',\n",
       " 'exorseiantolayelaulidinnaviellicianelettelenasj.',\n",
       " 'evontenisymoriantailynnasmiratersleennichanihaxiuistieanyickeynnakesseethumikamerolarennasaxyoseranienyzionaylynnandriamonzeettefffsvzbnesthawwyceyleovonaurnazikolioriracishdxnceyaniceninoodiyahimsonnirieliauelynnatheigabdumbekaelynzeyanikadqytannaversliustoniersosniximadililianavojulimielanieltuadielinirenlenaryseangtonaliavioiroshahrahirahartnelleetheruvalekksonisuherelynovanava.',\n",
       " 'exonbledighadenlesleileliaharjprufydiontavaligansharlieashalionslynettinelinasimusturamyriellicarinamakariovinnahkatheemilvelahaianaalissadavenjobilexceeliovaidekamarailleekeindriaherrornidencdkuy.']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.089372965115049\n"
     ]
    }
   ],
   "source": [
    "#evalutaing the log loss\n",
    "log_loss = 0\n",
    "n = 0\n",
    "\n",
    "for word in words:\n",
    "    word = '.' + word + '.'\n",
    "    for ch1, ch2, ch3 in zip(word, word[1:], word[2:]):\n",
    "        log_loss += torch.log(P[atoi[ch1], atoi[ch2], atoi[ch3]]).item() \n",
    "        n += 1\n",
    "log_loss /= (-n)\n",
    "print(log_loss)\n",
    "\n",
    "#a higher log loss as compared to a bigram model for the counting case, as there is more overhead\n",
    "#in the trigram model as we have only a few sensible pairs(based on the data) but lots of the\n",
    "#possible pairs are just getting a initial count of 0 and 1 later after smoothening, which was not \n",
    "#the case in bigram model where the overhead bw sensible chars and all the chars was very less\n",
    "\n",
    "#but in theory trigram models shld perfrom better than bigram models as they have more context, and\n",
    "#this will be seen in the neural network apporoach, which is how it is meant to be done in practice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs1, xs2, ys = [], [], []\n",
    "for word in words:\n",
    "    word = '.' + word + '.'\n",
    "    for ch1, ch2, ch3 in zip(word, word[1:], word[2:]):\n",
    "        xs1.append(atoi[ch1])\n",
    "        xs2.append(atoi[ch2])\n",
    "        ys.append(atoi[ch3])\n",
    "\n",
    "xs1 = torch.tensor(xs1)\n",
    "xs2 = torch.tensor(xs2) \n",
    "ys = torch.tensor(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1enc = F.one_hot(xs1, num_classes=27).float()\n",
    "x2enc = F.one_hot(xs2, num_classes=27).float()  \n",
    "yenc = F.one_hot(ys, num_classes=27).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a xenc vector for the input which is x1enc appeneded to x2enc inner vector wise, \n",
    "#giving us a 54 dim vector for each input, have to use this as opposed to 27 neuron input\n",
    "#as then it would lose sequential info, as ab and ba say would have the same 27 dim vector repr of\n",
    "#011000....(27 of them) but they are different, so we need to use a 54 dim vector for each input\n",
    "xenc = torch.cat((x1enc, x2enc), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196113, 54])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((54, 27), requires_grad=True, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = xenc @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 4.2793498039245605\n",
      "epoch: 1 loss: 4.0857038497924805\n",
      "epoch: 2 loss: 3.9157466888427734\n",
      "epoch: 3 loss: 3.768465042114258\n",
      "epoch: 4 loss: 3.6420226097106934\n",
      "epoch: 5 loss: 3.5334808826446533\n",
      "epoch: 6 loss: 3.439603805541992\n",
      "epoch: 7 loss: 3.357686758041382\n",
      "epoch: 8 loss: 3.285736560821533\n",
      "epoch: 9 loss: 3.222252607345581\n",
      "epoch: 10 loss: 3.165989637374878\n",
      "epoch: 11 loss: 3.115856170654297\n",
      "epoch: 12 loss: 3.0708956718444824\n",
      "epoch: 13 loss: 3.0303025245666504\n",
      "epoch: 14 loss: 2.993421792984009\n",
      "epoch: 15 loss: 2.9597320556640625\n",
      "epoch: 16 loss: 2.928818702697754\n",
      "epoch: 17 loss: 2.9003472328186035\n",
      "epoch: 18 loss: 2.874042510986328\n",
      "epoch: 19 loss: 2.8496744632720947\n",
      "epoch: 20 loss: 2.827047824859619\n",
      "epoch: 21 loss: 2.80599308013916\n",
      "epoch: 22 loss: 2.7863643169403076\n",
      "epoch: 23 loss: 2.7680325508117676\n",
      "epoch: 24 loss: 2.750882863998413\n",
      "epoch: 25 loss: 2.734813928604126\n",
      "epoch: 26 loss: 2.7197346687316895\n",
      "epoch: 27 loss: 2.705562114715576\n",
      "epoch: 28 loss: 2.6922221183776855\n",
      "epoch: 29 loss: 2.67964768409729\n",
      "epoch: 30 loss: 2.6677777767181396\n",
      "epoch: 31 loss: 2.6565561294555664\n",
      "epoch: 32 loss: 2.6459336280822754\n",
      "epoch: 33 loss: 2.6358628273010254\n",
      "epoch: 34 loss: 2.626302719116211\n",
      "epoch: 35 loss: 2.6172144412994385\n",
      "epoch: 36 loss: 2.608563184738159\n",
      "epoch: 37 loss: 2.6003172397613525\n",
      "epoch: 38 loss: 2.592446804046631\n",
      "epoch: 39 loss: 2.584925651550293\n",
      "epoch: 40 loss: 2.5777292251586914\n",
      "epoch: 41 loss: 2.5708351135253906\n",
      "epoch: 42 loss: 2.5642223358154297\n",
      "epoch: 43 loss: 2.557873010635376\n",
      "epoch: 44 loss: 2.551769495010376\n",
      "epoch: 45 loss: 2.545896291732788\n",
      "epoch: 46 loss: 2.540239095687866\n",
      "epoch: 47 loss: 2.5347847938537598\n",
      "epoch: 48 loss: 2.5295205116271973\n",
      "epoch: 49 loss: 2.5244362354278564\n",
      "epoch: 50 loss: 2.519521474838257\n",
      "epoch: 51 loss: 2.5147671699523926\n",
      "epoch: 52 loss: 2.510164499282837\n",
      "epoch: 53 loss: 2.5057053565979004\n",
      "epoch: 54 loss: 2.501382350921631\n",
      "epoch: 55 loss: 2.497189521789551\n",
      "epoch: 56 loss: 2.4931201934814453\n",
      "epoch: 57 loss: 2.4891676902770996\n",
      "epoch: 58 loss: 2.485328197479248\n",
      "epoch: 59 loss: 2.481595754623413\n",
      "epoch: 60 loss: 2.477965831756592\n",
      "epoch: 61 loss: 2.4744338989257812\n",
      "epoch: 62 loss: 2.470996141433716\n",
      "epoch: 63 loss: 2.46764874458313\n",
      "epoch: 64 loss: 2.464387893676758\n",
      "epoch: 65 loss: 2.461210012435913\n",
      "epoch: 66 loss: 2.4581127166748047\n",
      "epoch: 67 loss: 2.455091953277588\n",
      "epoch: 68 loss: 2.4521453380584717\n",
      "epoch: 69 loss: 2.449270248413086\n",
      "epoch: 70 loss: 2.446464776992798\n",
      "epoch: 71 loss: 2.443725347518921\n",
      "epoch: 72 loss: 2.441049814224243\n",
      "epoch: 73 loss: 2.43843674659729\n",
      "epoch: 74 loss: 2.4358835220336914\n",
      "epoch: 75 loss: 2.4333882331848145\n",
      "epoch: 76 loss: 2.4309494495391846\n",
      "epoch: 77 loss: 2.4285643100738525\n",
      "epoch: 78 loss: 2.42623233795166\n",
      "epoch: 79 loss: 2.4239511489868164\n",
      "epoch: 80 loss: 2.421719551086426\n",
      "epoch: 81 loss: 2.4195356369018555\n",
      "epoch: 82 loss: 2.417398452758789\n",
      "epoch: 83 loss: 2.4153060913085938\n",
      "epoch: 84 loss: 2.413257598876953\n",
      "epoch: 85 loss: 2.4112515449523926\n",
      "epoch: 86 loss: 2.4092869758605957\n",
      "epoch: 87 loss: 2.407362222671509\n",
      "epoch: 88 loss: 2.4054768085479736\n",
      "epoch: 89 loss: 2.4036290645599365\n",
      "epoch: 90 loss: 2.401818037033081\n",
      "epoch: 91 loss: 2.400043249130249\n",
      "epoch: 92 loss: 2.3983027935028076\n",
      "epoch: 93 loss: 2.396596908569336\n",
      "epoch: 94 loss: 2.3949239253997803\n",
      "epoch: 95 loss: 2.3932831287384033\n",
      "epoch: 96 loss: 2.3916738033294678\n",
      "epoch: 97 loss: 2.390094757080078\n",
      "epoch: 98 loss: 2.388545513153076\n",
      "epoch: 99 loss: 2.3870253562927246\n",
      "epoch: 100 loss: 2.385533094406128\n",
      "epoch: 101 loss: 2.384068727493286\n",
      "epoch: 102 loss: 2.3826310634613037\n",
      "epoch: 103 loss: 2.3812201023101807\n",
      "epoch: 104 loss: 2.3798344135284424\n",
      "epoch: 105 loss: 2.3784732818603516\n",
      "epoch: 106 loss: 2.3771369457244873\n",
      "epoch: 107 loss: 2.375823974609375\n",
      "epoch: 108 loss: 2.3745346069335938\n",
      "epoch: 109 loss: 2.373267412185669\n",
      "epoch: 110 loss: 2.3720226287841797\n",
      "epoch: 111 loss: 2.3707993030548096\n",
      "epoch: 112 loss: 2.369596481323242\n",
      "epoch: 113 loss: 2.368414878845215\n",
      "epoch: 114 loss: 2.367253065109253\n",
      "epoch: 115 loss: 2.3661108016967773\n",
      "epoch: 116 loss: 2.364987850189209\n",
      "epoch: 117 loss: 2.3638837337493896\n",
      "epoch: 118 loss: 2.362797737121582\n",
      "epoch: 119 loss: 2.361729383468628\n",
      "epoch: 120 loss: 2.3606789112091064\n",
      "epoch: 121 loss: 2.359645128250122\n",
      "epoch: 122 loss: 2.358628034591675\n",
      "epoch: 123 loss: 2.3576276302337646\n",
      "epoch: 124 loss: 2.356642723083496\n",
      "epoch: 125 loss: 2.3556740283966064\n",
      "epoch: 126 loss: 2.354719877243042\n",
      "epoch: 127 loss: 2.35378098487854\n",
      "epoch: 128 loss: 2.3528566360473633\n",
      "epoch: 129 loss: 2.3519465923309326\n",
      "epoch: 130 loss: 2.351050615310669\n",
      "epoch: 131 loss: 2.350167751312256\n",
      "epoch: 132 loss: 2.3492989540100098\n",
      "epoch: 133 loss: 2.348442792892456\n",
      "epoch: 134 loss: 2.3475992679595947\n",
      "epoch: 135 loss: 2.346768856048584\n",
      "epoch: 136 loss: 2.345950126647949\n",
      "epoch: 137 loss: 2.3451435565948486\n",
      "epoch: 138 loss: 2.344348669052124\n",
      "epoch: 139 loss: 2.3435659408569336\n",
      "epoch: 140 loss: 2.3427939414978027\n",
      "epoch: 141 loss: 2.3420331478118896\n",
      "epoch: 142 loss: 2.341282844543457\n",
      "epoch: 143 loss: 2.340543031692505\n",
      "epoch: 144 loss: 2.3398141860961914\n",
      "epoch: 145 loss: 2.3390953540802\n",
      "epoch: 146 loss: 2.338386058807373\n",
      "epoch: 147 loss: 2.337686777114868\n",
      "epoch: 148 loss: 2.3369972705841064\n",
      "epoch: 149 loss: 2.3363165855407715\n",
      "epoch: 150 loss: 2.3356451988220215\n",
      "epoch: 151 loss: 2.3349833488464355\n",
      "epoch: 152 loss: 2.334329843521118\n",
      "epoch: 153 loss: 2.3336853981018066\n",
      "epoch: 154 loss: 2.3330492973327637\n",
      "epoch: 155 loss: 2.33242130279541\n",
      "epoch: 156 loss: 2.331801414489746\n",
      "epoch: 157 loss: 2.3311901092529297\n",
      "epoch: 158 loss: 2.3305859565734863\n",
      "epoch: 159 loss: 2.3299901485443115\n",
      "epoch: 160 loss: 2.3294014930725098\n",
      "epoch: 161 loss: 2.328819751739502\n",
      "epoch: 162 loss: 2.3282463550567627\n",
      "epoch: 163 loss: 2.3276796340942383\n",
      "epoch: 164 loss: 2.327120304107666\n",
      "epoch: 165 loss: 2.3265674114227295\n",
      "epoch: 166 loss: 2.326021194458008\n",
      "epoch: 167 loss: 2.32548189163208\n",
      "epoch: 168 loss: 2.324949026107788\n",
      "epoch: 169 loss: 2.324422597885132\n",
      "epoch: 170 loss: 2.3239026069641113\n",
      "epoch: 171 loss: 2.3233883380889893\n",
      "epoch: 172 loss: 2.322880744934082\n",
      "epoch: 173 loss: 2.322378635406494\n",
      "epoch: 174 loss: 2.321882963180542\n",
      "epoch: 175 loss: 2.32139253616333\n",
      "epoch: 176 loss: 2.3209080696105957\n",
      "epoch: 177 loss: 2.3204286098480225\n",
      "epoch: 178 loss: 2.3199551105499268\n",
      "epoch: 179 loss: 2.319486618041992\n",
      "epoch: 180 loss: 2.319023847579956\n",
      "epoch: 181 loss: 2.318565845489502\n",
      "epoch: 182 loss: 2.3181135654449463\n",
      "epoch: 183 loss: 2.3176660537719727\n",
      "epoch: 184 loss: 2.317223072052002\n",
      "epoch: 185 loss: 2.3167853355407715\n",
      "epoch: 186 loss: 2.316352128982544\n",
      "epoch: 187 loss: 2.3159239292144775\n",
      "epoch: 188 loss: 2.315500259399414\n",
      "epoch: 189 loss: 2.3150808811187744\n",
      "epoch: 190 loss: 2.314666271209717\n",
      "epoch: 191 loss: 2.314255714416504\n",
      "epoch: 192 loss: 2.313849925994873\n",
      "epoch: 193 loss: 2.313448190689087\n",
      "epoch: 194 loss: 2.3130505084991455\n",
      "epoch: 195 loss: 2.312657117843628\n",
      "epoch: 196 loss: 2.312267780303955\n",
      "epoch: 197 loss: 2.311882257461548\n",
      "epoch: 198 loss: 2.3115012645721436\n",
      "epoch: 199 loss: 2.3111233711242676\n",
      "epoch: 200 loss: 2.3107497692108154\n",
      "epoch: 201 loss: 2.31037974357605\n",
      "epoch: 202 loss: 2.3100132942199707\n",
      "epoch: 203 loss: 2.3096508979797363\n",
      "epoch: 204 loss: 2.3092918395996094\n",
      "epoch: 205 loss: 2.308936357498169\n",
      "epoch: 206 loss: 2.308584213256836\n",
      "epoch: 207 loss: 2.3082358837127686\n",
      "epoch: 208 loss: 2.3078904151916504\n",
      "epoch: 209 loss: 2.3075485229492188\n",
      "epoch: 210 loss: 2.3072099685668945\n",
      "epoch: 211 loss: 2.3068745136260986\n",
      "epoch: 212 loss: 2.30654239654541\n",
      "epoch: 213 loss: 2.30621337890625\n",
      "epoch: 214 loss: 2.305887460708618\n",
      "epoch: 215 loss: 2.3055644035339355\n",
      "epoch: 216 loss: 2.3052446842193604\n",
      "epoch: 217 loss: 2.304927349090576\n",
      "epoch: 218 loss: 2.3046135902404785\n",
      "epoch: 219 loss: 2.30430269241333\n",
      "epoch: 220 loss: 2.3039939403533936\n",
      "epoch: 221 loss: 2.3036885261535645\n",
      "epoch: 222 loss: 2.3033862113952637\n",
      "epoch: 223 loss: 2.3030858039855957\n",
      "epoch: 224 loss: 2.302788496017456\n",
      "epoch: 225 loss: 2.3024940490722656\n",
      "epoch: 226 loss: 2.302201986312866\n",
      "epoch: 227 loss: 2.301912546157837\n",
      "epoch: 228 loss: 2.3016254901885986\n",
      "epoch: 229 loss: 2.3013412952423096\n",
      "epoch: 230 loss: 2.3010590076446533\n",
      "epoch: 231 loss: 2.3007798194885254\n",
      "epoch: 232 loss: 2.3005027770996094\n",
      "epoch: 233 loss: 2.3002278804779053\n",
      "epoch: 234 loss: 2.299955368041992\n",
      "epoch: 235 loss: 2.29968523979187\n",
      "epoch: 236 loss: 2.299417495727539\n",
      "epoch: 237 loss: 2.299152135848999\n",
      "epoch: 238 loss: 2.298888921737671\n",
      "epoch: 239 loss: 2.2986278533935547\n",
      "epoch: 240 loss: 2.2983686923980713\n",
      "epoch: 241 loss: 2.298111915588379\n",
      "epoch: 242 loss: 2.2978572845458984\n",
      "epoch: 243 loss: 2.29760479927063\n",
      "epoch: 244 loss: 2.297354221343994\n",
      "epoch: 245 loss: 2.2971057891845703\n",
      "epoch: 246 loss: 2.2968592643737793\n",
      "epoch: 247 loss: 2.2966148853302\n",
      "epoch: 248 loss: 2.296372175216675\n",
      "epoch: 249 loss: 2.2961318492889404\n",
      "epoch: 250 loss: 2.2958931922912598\n",
      "epoch: 251 loss: 2.295656204223633\n",
      "epoch: 252 loss: 2.295421600341797\n",
      "epoch: 253 loss: 2.2951884269714355\n",
      "epoch: 254 loss: 2.294956922531128\n",
      "epoch: 255 loss: 2.2947275638580322\n",
      "epoch: 256 loss: 2.2944998741149902\n",
      "epoch: 257 loss: 2.294274091720581\n",
      "epoch: 258 loss: 2.2940499782562256\n",
      "epoch: 259 loss: 2.293827533721924\n",
      "epoch: 260 loss: 2.293606758117676\n",
      "epoch: 261 loss: 2.2933876514434814\n",
      "epoch: 262 loss: 2.29317045211792\n",
      "epoch: 263 loss: 2.292954683303833\n",
      "epoch: 264 loss: 2.2927403450012207\n",
      "epoch: 265 loss: 2.292527914047241\n",
      "epoch: 266 loss: 2.2923169136047363\n",
      "epoch: 267 loss: 2.2921078205108643\n",
      "epoch: 268 loss: 2.2918996810913086\n",
      "epoch: 269 loss: 2.2916934490203857\n",
      "epoch: 270 loss: 2.2914888858795166\n",
      "epoch: 271 loss: 2.291285514831543\n",
      "epoch: 272 loss: 2.291083574295044\n",
      "epoch: 273 loss: 2.2908830642700195\n",
      "epoch: 274 loss: 2.290684461593628\n",
      "epoch: 275 loss: 2.2904868125915527\n",
      "epoch: 276 loss: 2.2902908325195312\n",
      "epoch: 277 loss: 2.2900960445404053\n",
      "epoch: 278 loss: 2.289902925491333\n",
      "epoch: 279 loss: 2.289710760116577\n",
      "epoch: 280 loss: 2.289520502090454\n",
      "epoch: 281 loss: 2.2893314361572266\n",
      "epoch: 282 loss: 2.2891430854797363\n",
      "epoch: 283 loss: 2.288956642150879\n",
      "epoch: 284 loss: 2.288771629333496\n",
      "epoch: 285 loss: 2.2885875701904297\n",
      "epoch: 286 loss: 2.2884044647216797\n",
      "epoch: 287 loss: 2.2882235050201416\n",
      "epoch: 288 loss: 2.2880430221557617\n",
      "epoch: 289 loss: 2.2878639698028564\n",
      "epoch: 290 loss: 2.287686347961426\n",
      "epoch: 291 loss: 2.2875099182128906\n",
      "epoch: 292 loss: 2.287334680557251\n",
      "epoch: 293 loss: 2.2871601581573486\n",
      "epoch: 294 loss: 2.2869873046875\n",
      "epoch: 295 loss: 2.2868154048919678\n",
      "epoch: 296 loss: 2.286644697189331\n",
      "epoch: 297 loss: 2.2864749431610107\n",
      "epoch: 298 loss: 2.286306619644165\n",
      "epoch: 299 loss: 2.2861392498016357\n",
      "epoch: 300 loss: 2.285973072052002\n",
      "epoch: 301 loss: 2.2858078479766846\n",
      "epoch: 302 loss: 2.2856438159942627\n",
      "epoch: 303 loss: 2.285480499267578\n",
      "epoch: 304 loss: 2.2853188514709473\n",
      "epoch: 305 loss: 2.2851579189300537\n",
      "epoch: 306 loss: 2.2849979400634766\n",
      "epoch: 307 loss: 2.284839153289795\n",
      "epoch: 308 loss: 2.284681558609009\n",
      "epoch: 309 loss: 2.28452467918396\n",
      "epoch: 310 loss: 2.2843687534332275\n",
      "epoch: 311 loss: 2.2842140197753906\n",
      "epoch: 312 loss: 2.28406023979187\n",
      "epoch: 313 loss: 2.283907175064087\n",
      "epoch: 314 loss: 2.283755302429199\n",
      "epoch: 315 loss: 2.283604383468628\n",
      "epoch: 316 loss: 2.283454179763794\n",
      "epoch: 317 loss: 2.2833054065704346\n",
      "epoch: 318 loss: 2.2831571102142334\n",
      "epoch: 319 loss: 2.2830095291137695\n",
      "epoch: 320 loss: 2.2828633785247803\n",
      "epoch: 321 loss: 2.2827181816101074\n",
      "epoch: 322 loss: 2.2825734615325928\n",
      "epoch: 323 loss: 2.2824296951293945\n",
      "epoch: 324 loss: 2.2822868824005127\n",
      "epoch: 325 loss: 2.282144784927368\n",
      "epoch: 326 loss: 2.282003879547119\n",
      "epoch: 327 loss: 2.2818636894226074\n",
      "epoch: 328 loss: 2.281723976135254\n",
      "epoch: 329 loss: 2.281585693359375\n",
      "epoch: 330 loss: 2.2814478874206543\n",
      "epoch: 331 loss: 2.28131103515625\n",
      "epoch: 332 loss: 2.281174898147583\n",
      "epoch: 333 loss: 2.2810397148132324\n",
      "epoch: 334 loss: 2.28090500831604\n",
      "epoch: 335 loss: 2.280771255493164\n",
      "epoch: 336 loss: 2.2806382179260254\n",
      "epoch: 337 loss: 2.2805063724517822\n",
      "epoch: 338 loss: 2.280374765396118\n",
      "epoch: 339 loss: 2.2802441120147705\n",
      "epoch: 340 loss: 2.2801144123077393\n",
      "epoch: 341 loss: 2.279985189437866\n",
      "epoch: 342 loss: 2.2798569202423096\n",
      "epoch: 343 loss: 2.2797293663024902\n",
      "epoch: 344 loss: 2.279602527618408\n",
      "epoch: 345 loss: 2.2794761657714844\n",
      "epoch: 346 loss: 2.279350757598877\n",
      "epoch: 347 loss: 2.279226064682007\n",
      "epoch: 348 loss: 2.279102087020874\n",
      "epoch: 349 loss: 2.2789785861968994\n",
      "epoch: 350 loss: 2.278855800628662\n",
      "epoch: 351 loss: 2.278733968734741\n",
      "epoch: 352 loss: 2.2786128520965576\n",
      "epoch: 353 loss: 2.2784922122955322\n",
      "epoch: 354 loss: 2.278372287750244\n",
      "epoch: 355 loss: 2.278252601623535\n",
      "epoch: 356 loss: 2.2781341075897217\n",
      "epoch: 357 loss: 2.2780163288116455\n",
      "epoch: 358 loss: 2.2778990268707275\n",
      "epoch: 359 loss: 2.2777822017669678\n",
      "epoch: 360 loss: 2.2776663303375244\n",
      "epoch: 361 loss: 2.2775509357452393\n",
      "epoch: 362 loss: 2.2774360179901123\n",
      "epoch: 363 loss: 2.2773220539093018\n",
      "epoch: 364 loss: 2.2772083282470703\n",
      "epoch: 365 loss: 2.2770955562591553\n",
      "epoch: 366 loss: 2.2769832611083984\n",
      "epoch: 367 loss: 2.2768714427948\n",
      "epoch: 368 loss: 2.2767603397369385\n",
      "epoch: 369 loss: 2.2766499519348145\n",
      "epoch: 370 loss: 2.2765398025512695\n",
      "epoch: 371 loss: 2.276430368423462\n",
      "epoch: 372 loss: 2.2763216495513916\n",
      "epoch: 373 loss: 2.2762134075164795\n",
      "epoch: 374 loss: 2.2761058807373047\n",
      "epoch: 375 loss: 2.275998592376709\n",
      "epoch: 376 loss: 2.275892496109009\n",
      "epoch: 377 loss: 2.2757863998413086\n",
      "epoch: 378 loss: 2.2756807804107666\n",
      "epoch: 379 loss: 2.275575876235962\n",
      "epoch: 380 loss: 2.2754719257354736\n",
      "epoch: 381 loss: 2.2753677368164062\n",
      "epoch: 382 loss: 2.2752647399902344\n",
      "epoch: 383 loss: 2.2751617431640625\n",
      "epoch: 384 loss: 2.275059700012207\n",
      "epoch: 385 loss: 2.2749581336975098\n",
      "epoch: 386 loss: 2.2748568058013916\n",
      "epoch: 387 loss: 2.2747561931610107\n",
      "epoch: 388 loss: 2.274656057357788\n",
      "epoch: 389 loss: 2.2745563983917236\n",
      "epoch: 390 loss: 2.2744574546813965\n",
      "epoch: 391 loss: 2.2743582725524902\n",
      "epoch: 392 loss: 2.2742605209350586\n",
      "epoch: 393 loss: 2.274162769317627\n",
      "epoch: 394 loss: 2.2740654945373535\n",
      "epoch: 395 loss: 2.2739689350128174\n",
      "epoch: 396 loss: 2.2738726139068604\n",
      "epoch: 397 loss: 2.2737770080566406\n",
      "epoch: 398 loss: 2.273681879043579\n",
      "epoch: 399 loss: 2.2735869884490967\n",
      "epoch: 400 loss: 2.2734928131103516\n",
      "epoch: 401 loss: 2.2733988761901855\n",
      "epoch: 402 loss: 2.2733054161071777\n",
      "epoch: 403 loss: 2.273212432861328\n",
      "epoch: 404 loss: 2.2731199264526367\n",
      "epoch: 405 loss: 2.2730278968811035\n",
      "epoch: 406 loss: 2.2729361057281494\n",
      "epoch: 407 loss: 2.2728447914123535\n",
      "epoch: 408 loss: 2.272754430770874\n",
      "epoch: 409 loss: 2.2726638317108154\n",
      "epoch: 410 loss: 2.2725741863250732\n",
      "epoch: 411 loss: 2.272484540939331\n",
      "epoch: 412 loss: 2.2723958492279053\n",
      "epoch: 413 loss: 2.2723073959350586\n",
      "epoch: 414 loss: 2.272218704223633\n",
      "epoch: 415 loss: 2.2721312046051025\n",
      "epoch: 416 loss: 2.2720439434051514\n",
      "epoch: 417 loss: 2.2719569206237793\n",
      "epoch: 418 loss: 2.2718703746795654\n",
      "epoch: 419 loss: 2.2717840671539307\n",
      "epoch: 420 loss: 2.2716987133026123\n",
      "epoch: 421 loss: 2.2716128826141357\n",
      "epoch: 422 loss: 2.2715282440185547\n",
      "epoch: 423 loss: 2.2714438438415527\n",
      "epoch: 424 loss: 2.271359443664551\n",
      "epoch: 425 loss: 2.271275758743286\n",
      "epoch: 426 loss: 2.2711923122406006\n",
      "epoch: 427 loss: 2.2711093425750732\n",
      "epoch: 428 loss: 2.271026849746704\n",
      "epoch: 429 loss: 2.270944595336914\n",
      "epoch: 430 loss: 2.2708628177642822\n",
      "epoch: 431 loss: 2.2707810401916504\n",
      "epoch: 432 loss: 2.270699977874756\n",
      "epoch: 433 loss: 2.2706193923950195\n",
      "epoch: 434 loss: 2.2705390453338623\n",
      "epoch: 435 loss: 2.270458698272705\n",
      "epoch: 436 loss: 2.270379066467285\n",
      "epoch: 437 loss: 2.2702999114990234\n",
      "epoch: 438 loss: 2.2702207565307617\n",
      "epoch: 439 loss: 2.2701423168182373\n",
      "epoch: 440 loss: 2.270063877105713\n",
      "epoch: 441 loss: 2.2699859142303467\n",
      "epoch: 442 loss: 2.2699081897735596\n",
      "epoch: 443 loss: 2.2698311805725098\n",
      "epoch: 444 loss: 2.269754409790039\n",
      "epoch: 445 loss: 2.2696776390075684\n",
      "epoch: 446 loss: 2.269601583480835\n",
      "epoch: 447 loss: 2.2695255279541016\n",
      "epoch: 448 loss: 2.2694499492645264\n",
      "epoch: 449 loss: 2.2693746089935303\n",
      "epoch: 450 loss: 2.2692999839782715\n",
      "epoch: 451 loss: 2.2692251205444336\n",
      "epoch: 452 loss: 2.269150972366333\n",
      "epoch: 453 loss: 2.2690770626068115\n",
      "epoch: 454 loss: 2.269003391265869\n",
      "epoch: 455 loss: 2.268930196762085\n",
      "epoch: 456 loss: 2.268857002258301\n",
      "epoch: 457 loss: 2.268784523010254\n",
      "epoch: 458 loss: 2.268712043762207\n",
      "epoch: 459 loss: 2.2686398029327393\n",
      "epoch: 460 loss: 2.2685680389404297\n",
      "epoch: 461 loss: 2.268496513366699\n",
      "epoch: 462 loss: 2.268425226211548\n",
      "epoch: 463 loss: 2.2683544158935547\n",
      "epoch: 464 loss: 2.2682838439941406\n",
      "epoch: 465 loss: 2.2682137489318848\n",
      "epoch: 466 loss: 2.268143892288208\n",
      "epoch: 467 loss: 2.2680740356445312\n",
      "epoch: 468 loss: 2.2680046558380127\n",
      "epoch: 469 loss: 2.2679355144500732\n",
      "epoch: 470 loss: 2.267866611480713\n",
      "epoch: 471 loss: 2.2677979469299316\n",
      "epoch: 472 loss: 2.2677297592163086\n",
      "epoch: 473 loss: 2.2676615715026855\n",
      "epoch: 474 loss: 2.267594337463379\n",
      "epoch: 475 loss: 2.267526626586914\n",
      "epoch: 476 loss: 2.2674596309661865\n",
      "epoch: 477 loss: 2.267392635345459\n",
      "epoch: 478 loss: 2.2673261165618896\n",
      "epoch: 479 loss: 2.2672595977783203\n",
      "epoch: 480 loss: 2.267193555831909\n",
      "epoch: 481 loss: 2.2671279907226562\n",
      "epoch: 482 loss: 2.2670624256134033\n",
      "epoch: 483 loss: 2.2669970989227295\n",
      "epoch: 484 loss: 2.2669320106506348\n",
      "epoch: 485 loss: 2.2668673992156982\n",
      "epoch: 486 loss: 2.2668027877807617\n",
      "epoch: 487 loss: 2.2667386531829834\n",
      "epoch: 488 loss: 2.266674518585205\n",
      "epoch: 489 loss: 2.266610860824585\n",
      "epoch: 490 loss: 2.266547441482544\n",
      "epoch: 491 loss: 2.266484022140503\n",
      "epoch: 492 loss: 2.26642107963562\n",
      "epoch: 493 loss: 2.2663583755493164\n",
      "epoch: 494 loss: 2.266295909881592\n",
      "epoch: 495 loss: 2.2662336826324463\n",
      "epoch: 496 loss: 2.266171455383301\n",
      "epoch: 497 loss: 2.2661099433898926\n",
      "epoch: 498 loss: 2.2660481929779053\n",
      "epoch: 499 loss: 2.2659871578216553\n"
     ]
    }
   ],
   "source": [
    "#gradient descent\n",
    "for i in range(500):\n",
    "    #forward pass to get the loss\n",
    "    logits = xenc @ W\n",
    "    count = logits.exp()\n",
    "    probs = count/count.sum(1, keepdim=True)\n",
    "    loss = -probs[torch.arange(ys.shape[0]), ys].log().mean() + 0.001*(W**2).mean()\n",
    "    print(f\"epoch: {i} loss: {loss.item()}\")\n",
    "\n",
    "    #backward pass to get the gradients\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    #update the weights\n",
    "    W.data += -10*W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.1988,  2.4937,  0.0842,  ..., -0.7910,  0.9349,  0.4622],\n",
       "        [ 1.2136,  0.7223, -0.0515,  ..., -1.1134,  0.2545, -0.1763],\n",
       "        [ 0.0175,  1.2562, -0.9390,  ...,  0.4527,  0.5984, -1.0554],\n",
       "        ...,\n",
       "        [ 1.0563,  0.5597, -0.9774,  ..., -0.3570,  0.7918, -0.1560],\n",
       "        [ 1.5444,  1.5410, -1.4410,  ..., -1.0210, -1.9105, -0.7989],\n",
       "        [ 0.3553,  1.9707,  0.3501,  ..., -1.1561,  1.2000, -0.7681]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so over the span of 200 epochs, the loss is lower than bigram NN model for the same num of \n",
    "#epochs, so trigram models are better than bigram models in theory and in practice(the NN model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating names now\n",
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_layer(ix1, ix2):\n",
    "    xenc = torch.cat((F.one_hot(torch.tensor([ix1]), num_classes=27).float(), F.one_hot(torch.tensor([ix2]), num_classes=27).float()), 1)\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts/counts.sum(1, keepdim=True)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errsolaycencelahersamesarymorickionyxknfbjrgartlyjaycehukeirayahanashviressmaphtonanamonyahivestynnyianewenaidesep.\n",
      "mpbrettaleyssantynczvsuhamukwaylilahccmsidchumbrielenedrinahiessamelygamick.\n",
      "ydebetqamiryletteencenavorynahrukaidalalaiciandiellamillornaziemmanslannatrougbmzioramiliellishakaizadendriannamalynnevaepqr.\n",
      "mhiandrelleannaledwbbramirlorianaisdvieliemuhajahiranierosalinavikaylaileytheodaelynnasthibpmkmpurhfzalrkelizie.\n",
      "csxcxa.\n"
     ]
    }
   ],
   "source": [
    "new_names = []\n",
    "for _ in range(5):\n",
    "    char = []\n",
    "    ix1 = 0\n",
    "    ix2 = 0\n",
    "    # char.append(itoa[ix2])\n",
    "    temp = 0\n",
    "    while True:\n",
    "        probs = output_layer(ix1, ix2)\n",
    "        temp = ix2\n",
    "        ix2 = torch.multinomial((P[ix1, ix2]), num_samples=1, replacement=True, generator=g).item()\n",
    "        ix1 = temp\n",
    "        char.append(itoa[ix2])\n",
    "        if ix2==0:\n",
    "            break\n",
    "    print(''.join(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
