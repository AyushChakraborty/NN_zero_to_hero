{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#at initialisation of weights the weights are more likely then not tuned in such a way that the probs\n",
    "#obtained at the very end is in a distribution where the intended outputs prob is very low and those\n",
    "#of the other are very high, hence more often then not, at the start when the model is untrained \n",
    "#the model is \"confidently wrong\", so the intention is to set the weights in such a way at the start\n",
    "#so that the all the logits are close to 0, and not in a way where some are extreme and other are not\n",
    "#so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when we start off with a \"confidently wrong\" model at the very start, we get very high losses\n",
    "#and for the first few epochs the aim of the model during training indirectly becomes to \n",
    "#squash/ make the logits uniform enough and then focus on rearranging the logits to make it match the\n",
    "#intended output, but the gain with having a model starting off in a intermediate stage(in the sense that\n",
    "#the initial logits are values close to 0) is that the number of training steps reqd are lesser and \n",
    "#we dont end up in a valley in the parameter-cost curve which does not have any local minima in its \n",
    "#proximity, which is not what we want, now in this case since we spend way more epochs rearranging the\n",
    "#logits to get the optimal trained model, and not squashing it for a lot of the first few epochs, \n",
    "#training becomes more productive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn',\n",
       " 'abigail',\n",
       " 'emily',\n",
       " 'elizabeth',\n",
       " 'mila',\n",
       " 'ella',\n",
       " 'avery',\n",
       " 'sofia',\n",
       " 'camila',\n",
       " 'aria',\n",
       " 'scarlett',\n",
       " 'victoria',\n",
       " 'madison',\n",
       " 'luna',\n",
       " 'grace',\n",
       " 'chloe',\n",
       " 'penelope',\n",
       " 'layla',\n",
       " 'riley',\n",
       " 'zoey',\n",
       " 'nora',\n",
       " 'lily',\n",
       " 'eleanor',\n",
       " 'hannah',\n",
       " 'lillian',\n",
       " 'addison',\n",
       " 'aubrey',\n",
       " 'ellie',\n",
       " 'stella',\n",
       " 'natalie',\n",
       " 'zoe',\n",
       " 'leah',\n",
       " 'hazel',\n",
       " 'violet',\n",
       " 'aurora',\n",
       " 'savannah',\n",
       " 'audrey',\n",
       " 'brooklyn',\n",
       " 'bella',\n",
       " 'claire',\n",
       " 'skylar',\n",
       " 'lucy',\n",
       " 'paisley',\n",
       " 'everly',\n",
       " 'anna',\n",
       " 'caroline',\n",
       " 'nova',\n",
       " 'genesis',\n",
       " 'emilia',\n",
       " 'kennedy',\n",
       " 'samantha',\n",
       " 'maya',\n",
       " 'willow',\n",
       " 'kinsley',\n",
       " 'naomi',\n",
       " 'aaliyah',\n",
       " 'elena',\n",
       " 'sarah',\n",
       " 'ariana',\n",
       " 'allison',\n",
       " 'gabriella',\n",
       " 'alice',\n",
       " 'madelyn',\n",
       " 'cora',\n",
       " 'ruby',\n",
       " 'eva',\n",
       " 'serenity',\n",
       " 'autumn',\n",
       " 'adeline',\n",
       " 'hailey',\n",
       " 'gianna',\n",
       " 'valentina',\n",
       " 'isla',\n",
       " 'eliana',\n",
       " 'quinn',\n",
       " 'nevaeh',\n",
       " 'ivy',\n",
       " 'sadie',\n",
       " 'piper',\n",
       " 'lydia',\n",
       " 'alexa',\n",
       " 'josephine',\n",
       " 'emery',\n",
       " 'julia',\n",
       " 'delilah',\n",
       " 'arianna',\n",
       " 'vivian',\n",
       " 'kaylee',\n",
       " 'sophie',\n",
       " 'brielle',\n",
       " 'madeline',\n",
       " 'peyton',\n",
       " 'rylee',\n",
       " 'clara',\n",
       " 'hadley',\n",
       " 'melanie',\n",
       " 'mackenzie',\n",
       " 'reagan',\n",
       " 'adalynn',\n",
       " 'liliana',\n",
       " 'aubree',\n",
       " 'jade',\n",
       " 'katherine',\n",
       " 'isabelle',\n",
       " 'natalia',\n",
       " 'raelynn',\n",
       " 'maria',\n",
       " 'athena',\n",
       " 'ximena',\n",
       " 'arya',\n",
       " 'leilani',\n",
       " 'taylor',\n",
       " 'faith',\n",
       " 'rose',\n",
       " 'kylie',\n",
       " 'alexandra',\n",
       " 'mary',\n",
       " 'margaret',\n",
       " 'lyla',\n",
       " 'ashley',\n",
       " 'amaya',\n",
       " 'eliza',\n",
       " 'brianna',\n",
       " 'bailey',\n",
       " 'andrea',\n",
       " 'khloe',\n",
       " 'jasmine',\n",
       " 'melody',\n",
       " 'iris',\n",
       " 'isabel',\n",
       " 'norah',\n",
       " 'annabelle',\n",
       " 'valeria',\n",
       " 'emerson',\n",
       " 'adalyn',\n",
       " 'ryleigh',\n",
       " 'eden',\n",
       " 'emersyn',\n",
       " 'anastasia',\n",
       " 'kayla',\n",
       " 'alyssa',\n",
       " 'juliana',\n",
       " 'charlie',\n",
       " 'esther',\n",
       " 'ariel',\n",
       " 'cecilia',\n",
       " 'valerie',\n",
       " 'alina',\n",
       " 'molly',\n",
       " 'reese',\n",
       " 'aliyah',\n",
       " 'lilly',\n",
       " 'parker',\n",
       " 'finley',\n",
       " 'morgan',\n",
       " 'sydney',\n",
       " 'jordyn',\n",
       " 'eloise',\n",
       " 'trinity',\n",
       " 'daisy',\n",
       " 'kimberly',\n",
       " 'lauren',\n",
       " 'genevieve',\n",
       " 'sara',\n",
       " 'arabella',\n",
       " 'harmony',\n",
       " 'elise',\n",
       " 'remi',\n",
       " 'teagan',\n",
       " 'alexis',\n",
       " 'london',\n",
       " 'sloane',\n",
       " 'laila',\n",
       " 'lucia',\n",
       " 'diana',\n",
       " 'juliette',\n",
       " 'sienna',\n",
       " 'elliana',\n",
       " 'londyn',\n",
       " 'ayla',\n",
       " 'callie',\n",
       " 'gracie',\n",
       " 'josie',\n",
       " 'amara',\n",
       " 'jocelyn',\n",
       " 'daniela',\n",
       " 'everleigh',\n",
       " 'mya',\n",
       " 'rachel',\n",
       " 'summer',\n",
       " 'alana',\n",
       " 'brooke',\n",
       " 'alaina',\n",
       " 'mckenzie',\n",
       " 'catherine',\n",
       " 'amy',\n",
       " 'presley',\n",
       " 'journee',\n",
       " 'rosalie',\n",
       " 'ember',\n",
       " 'brynlee',\n",
       " 'rowan',\n",
       " 'joanna',\n",
       " 'paige',\n",
       " 'rebecca',\n",
       " 'ana',\n",
       " 'sawyer',\n",
       " 'mariah',\n",
       " 'nicole',\n",
       " 'brooklynn',\n",
       " 'payton',\n",
       " 'marley',\n",
       " 'fiona',\n",
       " 'georgia',\n",
       " 'lila',\n",
       " 'harley',\n",
       " 'adelyn',\n",
       " 'alivia',\n",
       " 'noelle',\n",
       " 'gemma',\n",
       " 'vanessa',\n",
       " 'journey',\n",
       " 'makayla',\n",
       " 'angelina',\n",
       " 'adaline',\n",
       " 'catalina',\n",
       " 'alayna',\n",
       " 'julianna',\n",
       " 'leila',\n",
       " 'lola',\n",
       " 'adriana',\n",
       " 'june',\n",
       " 'juliet',\n",
       " 'jayla',\n",
       " 'river',\n",
       " 'tessa',\n",
       " 'lia',\n",
       " 'dakota',\n",
       " 'delaney',\n",
       " 'selena',\n",
       " 'blakely',\n",
       " 'ada',\n",
       " 'camille',\n",
       " 'zara',\n",
       " 'malia',\n",
       " 'hope',\n",
       " 'samara',\n",
       " 'vera',\n",
       " 'mckenna',\n",
       " 'briella',\n",
       " 'izabella',\n",
       " 'hayden',\n",
       " 'raegan',\n",
       " 'michelle',\n",
       " 'angela',\n",
       " 'ruth',\n",
       " 'freya',\n",
       " 'kamila',\n",
       " 'vivienne',\n",
       " 'aspen',\n",
       " 'olive',\n",
       " 'kendall',\n",
       " 'elaina',\n",
       " 'thea',\n",
       " 'kali',\n",
       " 'destiny',\n",
       " 'amiyah',\n",
       " 'evangeline',\n",
       " 'cali',\n",
       " 'blake',\n",
       " 'elsie',\n",
       " 'juniper',\n",
       " 'alexandria',\n",
       " 'myla',\n",
       " 'ariella',\n",
       " 'kate',\n",
       " 'mariana',\n",
       " 'lilah',\n",
       " 'charlee',\n",
       " 'daleyza',\n",
       " 'nyla',\n",
       " 'jane',\n",
       " 'maggie',\n",
       " 'zuri',\n",
       " 'aniyah',\n",
       " 'lucille',\n",
       " 'leia',\n",
       " 'melissa',\n",
       " 'adelaide',\n",
       " 'amina',\n",
       " 'giselle',\n",
       " 'lena',\n",
       " 'camilla',\n",
       " 'miriam',\n",
       " 'millie',\n",
       " 'brynn',\n",
       " 'gabrielle',\n",
       " 'sage',\n",
       " 'annie',\n",
       " 'logan',\n",
       " 'lilliana',\n",
       " 'haven',\n",
       " 'jessica',\n",
       " 'kaia',\n",
       " 'magnolia',\n",
       " 'amira',\n",
       " 'adelynn',\n",
       " 'makenzie',\n",
       " 'stephanie',\n",
       " 'nina',\n",
       " 'phoebe',\n",
       " 'arielle',\n",
       " 'evie',\n",
       " 'lyric',\n",
       " 'alessandra',\n",
       " 'gabriela',\n",
       " 'paislee',\n",
       " 'raelyn',\n",
       " 'madilyn',\n",
       " 'paris',\n",
       " 'makenna',\n",
       " 'kinley',\n",
       " 'gracelyn',\n",
       " 'talia',\n",
       " 'maeve',\n",
       " 'rylie',\n",
       " 'kiara',\n",
       " 'evelynn',\n",
       " 'brinley',\n",
       " 'jacqueline',\n",
       " 'laura',\n",
       " 'gracelynn',\n",
       " 'lexi',\n",
       " 'ariah',\n",
       " 'fatima',\n",
       " 'jennifer',\n",
       " 'kehlani',\n",
       " 'alani',\n",
       " 'ariyah',\n",
       " 'luciana',\n",
       " 'allie',\n",
       " 'heidi',\n",
       " 'maci',\n",
       " 'phoenix',\n",
       " 'felicity',\n",
       " 'joy',\n",
       " 'kenzie',\n",
       " 'veronica',\n",
       " 'margot',\n",
       " 'addilyn',\n",
       " 'lana',\n",
       " 'cassidy',\n",
       " 'remington',\n",
       " 'saylor',\n",
       " 'ryan',\n",
       " 'keira',\n",
       " 'harlow',\n",
       " 'miranda',\n",
       " 'angel',\n",
       " 'amanda',\n",
       " 'daniella',\n",
       " 'royalty',\n",
       " 'gwendolyn',\n",
       " 'ophelia',\n",
       " 'heaven',\n",
       " 'jordan',\n",
       " 'madeleine',\n",
       " 'esmeralda',\n",
       " 'kira',\n",
       " 'miracle',\n",
       " 'elle',\n",
       " 'amari',\n",
       " 'danielle',\n",
       " 'daphne',\n",
       " 'willa',\n",
       " 'haley',\n",
       " 'gia',\n",
       " 'kaitlyn',\n",
       " 'oakley',\n",
       " 'kailani',\n",
       " 'winter',\n",
       " 'alicia',\n",
       " 'serena',\n",
       " 'nadia',\n",
       " 'aviana',\n",
       " 'demi',\n",
       " 'jada',\n",
       " 'braelynn',\n",
       " 'dylan',\n",
       " 'ainsley',\n",
       " 'alison',\n",
       " 'camryn',\n",
       " 'avianna',\n",
       " 'bianca',\n",
       " 'skyler',\n",
       " 'scarlet',\n",
       " 'maddison',\n",
       " 'nylah',\n",
       " 'sarai',\n",
       " 'regina',\n",
       " 'dahlia',\n",
       " 'nayeli',\n",
       " 'raven',\n",
       " 'helen',\n",
       " 'adrianna',\n",
       " 'averie',\n",
       " 'skye',\n",
       " 'kelsey',\n",
       " 'tatum',\n",
       " 'kensley',\n",
       " 'maliyah',\n",
       " 'erin',\n",
       " 'viviana',\n",
       " 'jenna',\n",
       " 'anaya',\n",
       " 'carolina',\n",
       " 'shelby',\n",
       " 'sabrina',\n",
       " 'mikayla',\n",
       " 'annalise',\n",
       " 'octavia',\n",
       " 'lennon',\n",
       " 'blair',\n",
       " 'carmen',\n",
       " 'yaretzi',\n",
       " 'kennedi',\n",
       " 'mabel',\n",
       " 'zariah',\n",
       " 'kyla',\n",
       " 'christina',\n",
       " 'selah',\n",
       " 'celeste',\n",
       " 'eve',\n",
       " 'mckinley',\n",
       " 'milani',\n",
       " 'frances',\n",
       " 'jimena',\n",
       " 'kylee',\n",
       " 'leighton',\n",
       " 'katie',\n",
       " 'aitana',\n",
       " 'kayleigh',\n",
       " 'sierra',\n",
       " 'kathryn',\n",
       " 'rosemary',\n",
       " 'jolene',\n",
       " 'alondra',\n",
       " 'elisa',\n",
       " 'helena',\n",
       " 'charleigh',\n",
       " 'hallie',\n",
       " 'lainey',\n",
       " 'avah',\n",
       " 'jazlyn',\n",
       " 'kamryn',\n",
       " 'mira',\n",
       " 'cheyenne',\n",
       " 'francesca',\n",
       " 'antonella',\n",
       " 'wren',\n",
       " 'chelsea',\n",
       " 'amber',\n",
       " 'emory',\n",
       " 'lorelei',\n",
       " 'nia',\n",
       " 'abby',\n",
       " 'april',\n",
       " 'emelia',\n",
       " 'carter',\n",
       " 'aylin',\n",
       " 'cataleya',\n",
       " 'bethany',\n",
       " 'marlee',\n",
       " 'carly',\n",
       " 'kaylani',\n",
       " 'emely',\n",
       " 'liana',\n",
       " 'madelynn',\n",
       " 'cadence',\n",
       " 'matilda',\n",
       " 'sylvia',\n",
       " 'myra',\n",
       " 'fernanda',\n",
       " 'oaklyn',\n",
       " 'elianna',\n",
       " 'hattie',\n",
       " 'dayana',\n",
       " 'kendra',\n",
       " 'maisie',\n",
       " 'malaysia',\n",
       " 'kara',\n",
       " 'katelyn',\n",
       " 'maia',\n",
       " 'celine',\n",
       " 'cameron',\n",
       " 'renata',\n",
       " 'jayleen',\n",
       " 'charli',\n",
       " 'emmalyn',\n",
       " 'holly',\n",
       " 'azalea',\n",
       " 'leona',\n",
       " 'alejandra',\n",
       " 'bristol',\n",
       " 'collins',\n",
       " 'imani',\n",
       " 'meadow',\n",
       " 'alexia',\n",
       " 'edith',\n",
       " 'kaydence',\n",
       " 'leslie',\n",
       " 'lilith',\n",
       " 'kora',\n",
       " 'aisha',\n",
       " 'meredith',\n",
       " 'danna',\n",
       " 'wynter',\n",
       " 'emberly',\n",
       " 'julieta',\n",
       " 'michaela',\n",
       " 'alayah',\n",
       " 'jemma',\n",
       " 'reign',\n",
       " 'colette',\n",
       " 'kaliyah',\n",
       " 'elliott',\n",
       " 'johanna',\n",
       " 'remy',\n",
       " 'sutton',\n",
       " 'emmy',\n",
       " 'virginia',\n",
       " 'briana',\n",
       " 'oaklynn',\n",
       " 'adelina',\n",
       " 'everlee',\n",
       " 'megan',\n",
       " 'angelica',\n",
       " 'justice',\n",
       " 'mariam',\n",
       " 'khaleesi',\n",
       " 'macie',\n",
       " 'karsyn',\n",
       " 'alanna',\n",
       " 'aleah',\n",
       " 'mae',\n",
       " 'mallory',\n",
       " 'esme',\n",
       " 'skyla',\n",
       " 'madilynn',\n",
       " 'charley',\n",
       " 'allyson',\n",
       " 'hanna',\n",
       " 'shiloh',\n",
       " 'henley',\n",
       " 'macy',\n",
       " 'maryam',\n",
       " 'ivanna',\n",
       " 'ashlynn',\n",
       " 'lorelai',\n",
       " 'amora',\n",
       " 'ashlyn',\n",
       " 'sasha',\n",
       " 'baylee',\n",
       " 'beatrice',\n",
       " 'itzel',\n",
       " 'priscilla',\n",
       " 'marie',\n",
       " 'jayda',\n",
       " 'liberty',\n",
       " 'rory',\n",
       " 'alessia',\n",
       " 'alaia',\n",
       " 'janelle',\n",
       " 'kalani',\n",
       " 'gloria',\n",
       " 'sloan',\n",
       " 'dorothy',\n",
       " 'greta',\n",
       " 'julie',\n",
       " 'zahra',\n",
       " 'savanna',\n",
       " 'annabella',\n",
       " 'poppy',\n",
       " 'amalia',\n",
       " 'zaylee',\n",
       " 'cecelia',\n",
       " 'coraline',\n",
       " 'kimber',\n",
       " 'emmie',\n",
       " 'anne',\n",
       " 'karina',\n",
       " 'kassidy',\n",
       " 'kynlee',\n",
       " 'monroe',\n",
       " 'anahi',\n",
       " 'jaliyah',\n",
       " 'jazmin',\n",
       " 'maren',\n",
       " 'monica',\n",
       " 'siena',\n",
       " 'marilyn',\n",
       " 'reyna',\n",
       " 'kyra',\n",
       " 'lilian',\n",
       " 'jamie',\n",
       " 'melany',\n",
       " 'alaya',\n",
       " 'ariya',\n",
       " 'kelly',\n",
       " 'rosie',\n",
       " 'adley',\n",
       " 'dream',\n",
       " 'jaylah',\n",
       " 'laurel',\n",
       " 'jazmine',\n",
       " 'mina',\n",
       " 'karla',\n",
       " 'bailee',\n",
       " 'aubrie',\n",
       " 'katalina',\n",
       " 'melina',\n",
       " 'harlee',\n",
       " 'elliot',\n",
       " 'hayley',\n",
       " 'elaine',\n",
       " 'karen',\n",
       " 'dallas',\n",
       " 'irene',\n",
       " 'lylah',\n",
       " 'ivory',\n",
       " 'chaya',\n",
       " 'rosa',\n",
       " 'aleena',\n",
       " 'braelyn',\n",
       " 'nola',\n",
       " 'alma',\n",
       " 'leyla',\n",
       " 'pearl',\n",
       " 'addyson',\n",
       " 'roselyn',\n",
       " 'lacey',\n",
       " 'lennox',\n",
       " 'reina',\n",
       " 'aurelia',\n",
       " 'noa',\n",
       " 'janiyah',\n",
       " 'jessie',\n",
       " 'madisyn',\n",
       " 'saige',\n",
       " 'alia',\n",
       " 'tiana',\n",
       " 'astrid',\n",
       " 'cassandra',\n",
       " 'kyleigh',\n",
       " 'romina',\n",
       " 'stevie',\n",
       " 'haylee',\n",
       " 'zelda',\n",
       " 'lillie',\n",
       " 'aileen',\n",
       " 'brylee',\n",
       " 'eileen',\n",
       " 'yara',\n",
       " 'ensley',\n",
       " 'lauryn',\n",
       " 'giuliana',\n",
       " 'livia',\n",
       " 'anya',\n",
       " 'mikaela',\n",
       " 'palmer',\n",
       " 'lyra',\n",
       " 'mara',\n",
       " 'marina',\n",
       " 'kailey',\n",
       " 'liv',\n",
       " 'clementine',\n",
       " 'kenna',\n",
       " 'briar',\n",
       " 'emerie',\n",
       " 'galilea',\n",
       " 'tiffany',\n",
       " 'bonnie',\n",
       " 'elyse',\n",
       " 'cynthia',\n",
       " 'frida',\n",
       " 'kinslee',\n",
       " 'tatiana',\n",
       " 'joelle',\n",
       " 'armani',\n",
       " 'jolie',\n",
       " 'nalani',\n",
       " 'rayna',\n",
       " 'yareli',\n",
       " 'meghan',\n",
       " 'rebekah',\n",
       " 'addilynn',\n",
       " 'faye',\n",
       " 'zariyah',\n",
       " 'lea',\n",
       " 'aliza',\n",
       " 'julissa',\n",
       " 'lilyana',\n",
       " 'anika',\n",
       " 'kairi',\n",
       " 'aniya',\n",
       " 'noemi',\n",
       " 'angie',\n",
       " 'crystal',\n",
       " 'bridget',\n",
       " 'ari',\n",
       " 'davina',\n",
       " 'amelie',\n",
       " 'amirah',\n",
       " 'annika',\n",
       " 'elora',\n",
       " 'xiomara',\n",
       " 'linda',\n",
       " 'hana',\n",
       " 'laney',\n",
       " 'mercy',\n",
       " 'hadassah',\n",
       " 'madalyn',\n",
       " 'louisa',\n",
       " 'simone',\n",
       " 'kori',\n",
       " 'jillian',\n",
       " 'alena',\n",
       " 'malaya',\n",
       " 'miley',\n",
       " 'milan',\n",
       " 'sariyah',\n",
       " 'malani',\n",
       " 'clarissa',\n",
       " 'nala',\n",
       " 'princess',\n",
       " 'amani',\n",
       " 'analia',\n",
       " 'estella',\n",
       " 'milana',\n",
       " 'aya',\n",
       " 'chana',\n",
       " 'jayde',\n",
       " 'tenley',\n",
       " 'zaria',\n",
       " 'itzayana',\n",
       " 'penny',\n",
       " 'ailani',\n",
       " 'lara',\n",
       " 'aubriella',\n",
       " 'clare',\n",
       " 'lina',\n",
       " 'rhea',\n",
       " 'bria',\n",
       " 'thalia',\n",
       " 'keyla',\n",
       " 'haisley',\n",
       " 'ryann',\n",
       " 'addisyn',\n",
       " 'amaia',\n",
       " 'chanel',\n",
       " 'ellen',\n",
       " 'harmoni',\n",
       " 'aliana',\n",
       " 'tinsley',\n",
       " 'landry',\n",
       " 'paisleigh',\n",
       " 'lexie',\n",
       " 'myah',\n",
       " 'rylan',\n",
       " 'deborah',\n",
       " 'emilee',\n",
       " 'laylah',\n",
       " 'novalee',\n",
       " 'ellis',\n",
       " 'emmeline',\n",
       " 'avalynn',\n",
       " 'hadlee',\n",
       " 'legacy',\n",
       " 'braylee',\n",
       " 'elisabeth',\n",
       " 'kaylie',\n",
       " 'ansley',\n",
       " 'dior',\n",
       " 'paula',\n",
       " 'belen',\n",
       " 'corinne',\n",
       " 'maleah',\n",
       " 'martha',\n",
       " 'teresa',\n",
       " 'salma',\n",
       " 'louise',\n",
       " 'averi',\n",
       " 'lilianna',\n",
       " 'amiya',\n",
       " 'milena',\n",
       " 'royal',\n",
       " 'aubrielle',\n",
       " 'calliope',\n",
       " 'frankie',\n",
       " 'natasha',\n",
       " 'kamilah',\n",
       " 'meilani',\n",
       " 'raina',\n",
       " 'amayah',\n",
       " 'lailah',\n",
       " 'rayne',\n",
       " 'zaniyah',\n",
       " 'isabela',\n",
       " 'nathalie',\n",
       " 'miah',\n",
       " 'opal',\n",
       " 'kenia',\n",
       " 'azariah',\n",
       " 'hunter',\n",
       " 'tori',\n",
       " 'andi',\n",
       " 'keily',\n",
       " 'leanna',\n",
       " 'scarlette',\n",
       " 'jaelyn',\n",
       " 'saoirse',\n",
       " 'selene',\n",
       " 'dalary',\n",
       " 'lindsey',\n",
       " 'marianna',\n",
       " 'ramona',\n",
       " 'estelle',\n",
       " 'giovanna',\n",
       " 'holland',\n",
       " 'nancy',\n",
       " 'emmalynn',\n",
       " 'mylah',\n",
       " 'rosalee',\n",
       " 'sariah',\n",
       " 'zoie',\n",
       " 'blaire',\n",
       " 'lyanna',\n",
       " 'maxine',\n",
       " 'anais',\n",
       " 'dana',\n",
       " 'judith',\n",
       " 'kiera',\n",
       " 'jaelynn',\n",
       " 'noor',\n",
       " 'kai',\n",
       " 'adalee',\n",
       " 'oaklee',\n",
       " 'amaris',\n",
       " 'jaycee',\n",
       " 'belle',\n",
       " 'carolyn',\n",
       " 'della',\n",
       " 'karter',\n",
       " 'sky',\n",
       " 'treasure',\n",
       " 'vienna',\n",
       " 'jewel',\n",
       " 'rivka',\n",
       " 'rosalyn',\n",
       " 'alannah',\n",
       " 'ellianna',\n",
       " 'sunny',\n",
       " 'claudia',\n",
       " 'cara',\n",
       " 'hailee',\n",
       " 'estrella',\n",
       " 'harleigh',\n",
       " 'zhavia',\n",
       " 'alianna',\n",
       " 'brittany',\n",
       " 'jaylene',\n",
       " 'journi',\n",
       " 'marissa',\n",
       " 'mavis',\n",
       " 'iliana',\n",
       " 'jurnee',\n",
       " 'aislinn',\n",
       " 'alyson',\n",
       " 'elsa',\n",
       " 'kamiyah',\n",
       " 'kiana',\n",
       " 'lisa',\n",
       " 'arlette',\n",
       " 'kadence',\n",
       " 'kathleen',\n",
       " 'halle',\n",
       " 'erika',\n",
       " 'sylvie',\n",
       " 'adele',\n",
       " 'erica',\n",
       " 'veda',\n",
       " 'whitney',\n",
       " 'bexley',\n",
       " 'emmaline',\n",
       " 'guadalupe',\n",
       " 'august',\n",
       " 'brynleigh',\n",
       " 'gwen',\n",
       " 'promise',\n",
       " 'alisson',\n",
       " 'india',\n",
       " 'madalynn',\n",
       " 'paloma',\n",
       " 'patricia',\n",
       " 'samira',\n",
       " 'aliya',\n",
       " 'casey',\n",
       " 'jazlynn',\n",
       " 'paulina',\n",
       " 'dulce',\n",
       " 'kallie',\n",
       " 'perla',\n",
       " 'adrienne',\n",
       " 'alora',\n",
       " 'nataly',\n",
       " 'ayleen',\n",
       " 'christine',\n",
       " 'kaiya',\n",
       " 'ariadne',\n",
       " 'karlee',\n",
       " 'barbara',\n",
       " 'lillianna',\n",
       " 'raquel',\n",
       " 'saniyah',\n",
       " 'yamileth',\n",
       " 'arely',\n",
       " 'celia',\n",
       " 'heavenly',\n",
       " 'kaylin',\n",
       " 'marisol',\n",
       " 'marleigh',\n",
       " 'avalyn',\n",
       " 'berkley',\n",
       " 'kataleya',\n",
       " 'zainab',\n",
       " 'dani',\n",
       " 'egypt',\n",
       " 'joyce',\n",
       " 'kenley',\n",
       " 'annabel',\n",
       " 'kaelyn',\n",
       " 'etta',\n",
       " 'hadleigh',\n",
       " 'joselyn',\n",
       " 'luella',\n",
       " 'jaylee',\n",
       " 'zola',\n",
       " 'alisha',\n",
       " 'ezra',\n",
       " 'queen',\n",
       " 'amia',\n",
       " 'annalee',\n",
       " 'bellamy',\n",
       " 'paola',\n",
       " 'tinley',\n",
       " 'violeta',\n",
       " 'jenesis',\n",
       " 'arden',\n",
       " 'giana',\n",
       " 'wendy',\n",
       " 'ellison',\n",
       " 'florence',\n",
       " 'margo',\n",
       " 'naya',\n",
       " 'robin',\n",
       " 'sandra',\n",
       " 'scout',\n",
       " 'waverly',\n",
       " 'janessa',\n",
       " 'jayden',\n",
       " 'micah',\n",
       " 'novah',\n",
       " 'zora',\n",
       " 'ann',\n",
       " 'jana',\n",
       " 'taliyah',\n",
       " 'vada',\n",
       " 'giavanna',\n",
       " 'ingrid',\n",
       " 'valery',\n",
       " 'azaria',\n",
       " 'emmarie',\n",
       " 'esperanza',\n",
       " 'kailyn',\n",
       " 'aiyana',\n",
       " 'keilani',\n",
       " 'austyn',\n",
       " 'whitley',\n",
       " 'elina',\n",
       " 'kimora',\n",
       " 'maliah',\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "\n",
    "atoi = {ch: i+1 for i, ch in enumerate(chars)}\n",
    "atoi['.'] = 0\n",
    "\n",
    "itoa = {i: ch for ch, i in atoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in (w+'.'):\n",
    "            ix = atoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))   #splitting of the datset of words, then from those split datasets we make the dataset for our use\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "\n",
    "Xval, Yval = build_dataset(words[n1:n2])\n",
    "\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([182625, 3]), torch.Size([182625]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape, Ytr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init values of weights and biases, since we are also modifying the tensors separately, we\n",
    "#set req graident to be true later on\n",
    "C = torch.randn([27, 10], generator=g)\n",
    "W1 = torch.randn([30, 200], generator=g) * (5/3)/(30)**0.5  #this is the process of kaiming\n",
    "#init from the paper kaiming et al 2015 which delves into rectifiers/act funcs, here the fundamental\n",
    "#is that we want the values coming out from any layer at init only to have mean of 0 and std of 1\n",
    "#now w@x + b might not always have std of 1 but a higher value, assuming x and b are also normal\n",
    "#so to offset that we multiply the init val of w by gain/rt(fan_in)  where gain depends on the type\n",
    "#of non linear activation used and fan_in is the number of neurons which feed itself in into the \n",
    "#current layer, gain for tanh activation is 5/3, for ReLU its rt(2) and for the others generic ones its 1\n",
    "\n",
    "\n",
    "#b1 = torch.rand(200, generator=g) * 0.01  #so as to make the num which goes in the tanh func be close to \n",
    "#0 which prevents it from hitting the dead state\n",
    "W2 = torch.randn([200, 27], generator=g) * 0.01\n",
    "b2 = torch.rand(27, generator=g) * 0\n",
    "\n",
    "bngain = torch.ones((1, 200))\n",
    "bnbias = torch.zeros((1, 200))\n",
    " \n",
    "bnmean_running = torch.zeros((1, 200))\n",
    "bnstd_running = torch.ones((1, 200))\n",
    "\n",
    "parameters = [C, W1, W2, b2, bnbias, bngain]\n",
    "\n",
    "#now setting require_grad to be true\n",
    "for p in parameters:\n",
    "    p.requires_grad = True;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 1: 3.2742714881896973\n",
      "loss at epoch 10001: 2.1143321990966797\n",
      "loss at epoch 20001: 2.3284220695495605\n",
      "loss at epoch 30001: 2.181339979171753\n",
      "loss at epoch 40001: 2.179100275039673\n",
      "loss at epoch 50001: 1.8406212329864502\n",
      "loss at epoch 60001: 2.0422685146331787\n",
      "loss at epoch 70001: 2.2556426525115967\n",
      "loss at epoch 80001: 2.310425043106079\n",
      "loss at epoch 90001: 2.034607172012329\n",
      "loss at epoch 100001: 2.2446584701538086\n",
      "loss at epoch 110001: 1.8314517736434937\n",
      "loss at epoch 120001: 2.0750114917755127\n",
      "loss at epoch 130001: 2.1405625343322754\n",
      "loss at epoch 140001: 2.1285812854766846\n",
      "loss at epoch 150001: 1.8639581203460693\n",
      "loss at epoch 160001: 1.9118980169296265\n",
      "loss at epoch 170001: 1.7090238332748413\n",
      "loss at epoch 180001: 1.8510971069335938\n",
      "loss at epoch 190001: 2.0543015003204346\n"
     ]
    }
   ],
   "source": [
    "#gradient descent step\n",
    "for _ in range(200000):\n",
    "    #minibatch of 32 batches selected randomly\n",
    "    ix = torch.randint(0, Xtr.shape[0], [32]) \n",
    "\n",
    "    #forawrd pass\n",
    "    emb = C[Xtr[ix]]     #embedding the context\n",
    "    hpreact = emb.view(-1, W1.shape[0]) @ W1 #+ b1 no need of this bias as its being subtracted\n",
    "    #later on while performing batchNorm, instead bnbias takes over its role\n",
    "    \n",
    "    #batchnorm step\n",
    "    bnmean = hpreact.mean(0, keepdim=True)\n",
    "    bnstd = hpreact.std(0, keepdim=True)\n",
    "    hpreact = bngain * (hpreact - bnmean / bnstd) + bnbias\n",
    "\n",
    "    with torch.no_grad():\n",
    "        bnmean_running = 0.999*bnmean_running + 0.001*bnmean  #only makes sense because if we \n",
    "        #just add the new means every time the values would be some blown up number and not even\n",
    "        #something which might resemble the mean for all of the samples, hence this is done. Also\n",
    "        #we cant take the final value only as all of them are taken from samples and we cant assume\n",
    "        #that the final sample is the most important one, hence again, this approach of partial \n",
    "        #increment is the most sensible one, and the same goes for std. Here also this value of 0.001\n",
    "        #is the momentum, where if we have smaller batch sizes, then its good to have smaller \n",
    "        #momentum, as for smaller batch sizes the mean and std could vary around a lot, and to \n",
    "        #make the val of mean/std converge, we incremenet the changes but by a small step, but for \n",
    "        #larger batch sizes, we expect the mean/std to not vary by a lot each time, hence the \n",
    "        #momentum can be large enough.\n",
    "        bnstd_running = 0.999*bnstd_running + 0.001*bnstd\n",
    "\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    nll = F.cross_entropy(logits, Ytr[ix])\n",
    "    if (_%10000 == 0):\n",
    "        print(f\"loss at epoch {_+1}: {nll.item()}\")   #loss for just that minibatch\n",
    "     \n",
    "    #backward pass\n",
    "    #setting all the params grads to 0 before backpropagating\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    nll.backward()  #backpropagating the loss\n",
    "\n",
    "    #update the parameters\n",
    "    lr = 0.1 if _ <100000 else 0.01\n",
    "    for p in parameters: \n",
    "        if p.grad is not None:\n",
    "            p.data += -lr * p.grad\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 2.0459465980529785\n",
      "test loss: 2.1061246395111084\n",
      "val loss: 2.10913348197937\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad\n",
    "def split_loss(split):\n",
    "    x, y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'test': (Xte, Yte),\n",
    "        'val': (Xval, Yval)\n",
    "    }[split]\n",
    "    emb = C[x]\n",
    "    hpreact = emb.view(-1, W1.shape[0]) @ W1 + b1\n",
    "    hpreact = bngain * (hpreact - bnmean_running / bnstd_running) + bnbias\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    nll = F.cross_entropy(logits, y)\n",
    "    print(f\"{split} loss: {nll}\")\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('test')\n",
    "split_loss('val')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating generlisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a general linear layer\n",
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5  #kaiming init of weights\n",
    "        self.bias = torch.randn((fan_out), generator=g) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias]) \n",
    "    \n",
    "\n",
    "\n",
    "#a general batchNorm layer\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, momentum=0.1, eps=1e-5):  #dim is the num of neurons in the layer\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "\n",
    "    def __call__(self, x):  #x is the samples of the batch \n",
    "        if self.training:\n",
    "            xmean = x.mean(0, keepdim=True)\n",
    "            xvar = x.var(0, keepdim=True)\n",
    "        else:  #the case of inference where running mean and std is used for inference\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "\n",
    "        self.out = self.gamma * (x - xmean/torch.sqrt(xvar + self.eps)) + self.beta\n",
    "\n",
    "        if self.training:  #if training occurs, then keep updating the running mean and std\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1-self.momentum)*self.running_mean + self.momentum*xmean\n",
    "                self.running_var = (1-self.momentum)*self.running_var + self.momentum*xvar\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "\n",
    "#a general tanh layer\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47551\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 27       #27 chars to encode \n",
    "block_size = 3        #3 chars as context \n",
    "n_embd = 10           #dimensionality of embedding space\n",
    "n_hidden = 100        #number of dimensions of hidden layer\n",
    "g = torch.Generator().manual_seed(2147483644)   #generator for uniformity of randomness across trials\n",
    "\n",
    "C = torch.rand((vocab_size, n_embd), generator=g)\n",
    "layers = [\n",
    "    Linear(n_embd * block_size, n_hidden), \n",
    "    BatchNorm1d(n_hidden),\n",
    "    Tanh(),\n",
    "    Linear(n_hidden, n_hidden), \n",
    "    BatchNorm1d(n_hidden),\n",
    "    Tanh(),\n",
    "    Linear(n_hidden, n_hidden),\n",
    "    BatchNorm1d(n_hidden), \n",
    "    Tanh(),\n",
    "    Linear(n_hidden, n_hidden), \n",
    "    BatchNorm1d(n_hidden),\n",
    "    Tanh(),\n",
    "    Linear(n_hidden, n_hidden), \n",
    "    BatchNorm1d(n_hidden),\n",
    "    Tanh(),\n",
    "    Linear(n_hidden, vocab_size), \n",
    "    BatchNorm1d(vocab_size)\n",
    "]\n",
    "#by default, all of the weights are kaiming initialised, where for the init case, the num of \n",
    "#neurons in the saturated region(which makes the neuron after passing through the non linear act func dead)\n",
    "#are lower\n",
    "\n",
    "#some more init setting up of weights\n",
    "with torch.no_grad():\n",
    "    #letting the logits layer be less confidently wrong by letting the last layer of weights be\n",
    "    #scaled down by a factor of 0.1\n",
    "    layers[-1].gamma *= 0.1\n",
    "\n",
    "    #also since we apply the tanh act func to all the *hidden layer* , we need to multiply the \n",
    "    #kaiming init weights by its appropriate gain of 5/3\n",
    "    for layer in layers[:-1]:  #excluding the last output layer\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.weight *= 5/3  #and again 5/3 is used as a gain for the weights used in \n",
    "            #the linear layer as the tanh func squishes all the values close to 0, so\n",
    "            #over multiple layers, this results in more and more squished vals, and so as\n",
    "            #to combat it to a certain extent, we boost it by the gain\n",
    "    \n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "print(sum(p.nelement() for p in parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.5358\n",
      "  10000/ 200000: 2.2921\n",
      "  20000/ 200000: 2.0982\n",
      "  30000/ 200000: 2.0721\n",
      "  40000/ 200000: 2.2450\n",
      "  50000/ 200000: 1.9345\n",
      "  60000/ 200000: 2.0556\n",
      "  70000/ 200000: 1.9227\n",
      "  80000/ 200000: 2.1679\n",
      "  90000/ 200000: 1.9471\n",
      " 100000/ 200000: 2.4487\n",
      " 110000/ 200000: 2.1672\n",
      " 120000/ 200000: 1.9262\n",
      " 130000/ 200000: 2.0644\n",
      " 140000/ 200000: 2.3572\n",
      " 150000/ 200000: 1.7786\n",
      " 160000/ 200000: 2.1733\n",
      " 170000/ 200000: 1.8967\n",
      " 180000/ 200000: 1.9712\n",
      " 190000/ 200000: 1.8412\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 200000\n",
    "batch_size = 32\n",
    "lossi = []  #appending the log(nll) for each epoch \n",
    "\n",
    "for i in range(max_epochs):\n",
    "    #making the minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size, ), generator=g)  #get index of chosen samples\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]  #batch X, Y\n",
    "\n",
    "    #forward pass\n",
    "    emb = C[Xb]  #char embedding\n",
    "    x = emb.view(emb.shape[0], -1)  #this is the input layer essentially of shape (batch_size, )\n",
    "    for layer in layers:\n",
    "        x = layer(x)  #egs passed from one layer to the other \n",
    "    nll = F.cross_entropy(x, Yb)  #loss\n",
    "\n",
    "    #backward pass\n",
    "    for layer in layers: #capturing the grads\n",
    "        layer.out.retain_grad()\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    nll.backward()\n",
    "\n",
    "    #update\n",
    "    alpha = 0.1 if i <100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data -= alpha*p.grad\n",
    "\n",
    "    #track stats\n",
    "    if i%10000 == 0:\n",
    "        print(f\"{i:7d}/{max_epochs:7d}: {nll.item():.4f}\")\n",
    "    lossi.append(nll.log10().item())   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
